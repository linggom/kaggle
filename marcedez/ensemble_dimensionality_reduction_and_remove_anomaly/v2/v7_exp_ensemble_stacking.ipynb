{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator,TransformerMixin, ClassifierMixin\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.linear_model import ElasticNetCV, LassoLarsCV, LinearRegression, HuberRegressor, RANSACRegressor,BayesianRidge\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StackingEstimator(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, estimator):\n",
    "        self.estimator = estimator\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        self.estimator.fit(X, y, **fit_params)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X = check_array(X)\n",
    "        X_transformed = np.copy(X)\n",
    "        # add class probabilities as a synthetic feature\n",
    "        if issubclass(self.estimator.__class__, ClassifierMixin) and hasattr(self.estimator, 'predict_proba'):\n",
    "            X_transformed = np.hstack((self.estimator.predict_proba(X), X))\n",
    "\n",
    "        # add class prodiction as a synthetic feature\n",
    "        X_transformed = np.hstack((np.reshape(self.estimator.predict(X), (-1, 1)), X_transformed))\n",
    "\n",
    "        return X_transformed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probing = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "if probing:\n",
    "    train = pd.read_csv('../../data/train2.csv')    \n",
    "else:\n",
    "    train = pd.read_csv('../../data/train.csv')\n",
    "\n",
    "test = pd.read_csv('../../data/test.csv')\n",
    "original_train = pd.read_csv('../../data/train.csv')\n",
    "columns = original_train.drop('y', axis=1).columns\n",
    "\n",
    "public_lb = pd.read_csv('../../data/public_lb.csv')\n",
    "public_lb['ID'] = public_lb.id\n",
    "public_lb['y'] = public_lb.yValue\n",
    "\n",
    "best = pd.read_csv('v7.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(train, test, field):\n",
    "    y_group = train.groupby('X0')['X0', 'y'].mean()\n",
    "    y_mean = np.mean(train.y)\n",
    "    mp_X0_g = {}\n",
    "    x = []\n",
    "    idx = y_group.index\n",
    "    val = y_group.values\n",
    "    for yg in range(y_group.shape[0]):\n",
    "        mp_X0_g[idx[yg]] = y_group.values[yg]\n",
    "\n",
    "    g = [ mp_X0_g.get(g, y_mean) for g in train.X0.values]\n",
    "    train['M%s' % field] = g\n",
    "    g = [ mp_X0_g.get(g, y_mean) for g in test.X0.values]\n",
    "    test['M%s' % field] = g\n",
    "    return train, test\n",
    "train, test = generate(train, test, 'X0')\n",
    "train, test = generate(train, test, 'X1')\n",
    "train, test = generate(train, test, 'X2')\n",
    "train, test = generate(train, test, 'X3')\n",
    "train, test = generate(train, test, 'X4')\n",
    "train, test = generate(train, test, 'X5')\n",
    "train, test = generate(train, test, 'X6')\n",
    "train, test = generate(train, test, 'X8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['M9'] = train[columns[9:]].mean(axis=1)\n",
    "test['M9'] = test[columns[9:]].mean(axis=1)\n",
    "train['M10'] = train[columns[9:]].median(axis=1)\n",
    "test['M10'] = test[columns[9:]].median(axis=1)\n",
    "train['M11'] = train[columns[9:]].std(axis=1)\n",
    "test['M11'] = test[columns[9:]].std(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "usable_columns = train.columns[:1].append(train.columns[2:])\n",
    "usable_columns\n",
    "# usable_columns = columns[9:]\n",
    "# usable_columns = ['ID', 'MX0', 'MX1', 'MX2', 'MX3', 'MX4', 'MX5', 'MX6', 'MX8', 'M9', 'M10', 'M11', 'X47','X95','X314','X315','X232','X119','X311','X76','X329','X238','X340','X362','X137']\n",
    "# usable_columns=['ID', 'X0', 'X47','X95','X314','X315','X232','X119','X311','X76','X329','X238','X340','X362','X137']\n",
    "usable_columns = ['X118',\n",
    "            'X127',\n",
    "            'X47',\n",
    "            'X315',\n",
    "            'X311',\n",
    "            'X179',\n",
    "            'X314',\n",
    "### added by Tilii\n",
    "            'X232',\n",
    "            'X29',\n",
    "            'X263',\n",
    "###\n",
    "            'X261']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for c in train.columns:\n",
    "    if train[c].dtype == 'object':\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(train[c].values) + list(test[c].values))\n",
    "        train[c] = lbl.transform(list(train[c].values))\n",
    "        test[c] = lbl.transform(list(test[c].values))\n",
    "\n",
    "\n",
    "\n",
    "n_comp = 12\n",
    "\n",
    "# tSVD\n",
    "tsvd = TruncatedSVD(n_components=n_comp, random_state=420)\n",
    "tsvd_results_train = tsvd.fit_transform(train.drop([\"y\"], axis=1))\n",
    "tsvd_results_test = tsvd.transform(test)\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=n_comp, random_state=420)\n",
    "pca2_results_train = pca.fit_transform(train.drop([\"y\"], axis=1))\n",
    "pca2_results_test = pca.transform(test)\n",
    "\n",
    "# ICA\n",
    "ica = FastICA(n_components=n_comp, random_state=420)\n",
    "ica2_results_train = ica.fit_transform(train.drop([\"y\"], axis=1))\n",
    "ica2_results_test = ica.transform(test)\n",
    "\n",
    "# GRP\n",
    "grp = GaussianRandomProjection(n_components=n_comp, eps=0.1, random_state=420)\n",
    "grp_results_train = grp.fit_transform(train.drop([\"y\"], axis=1))\n",
    "grp_results_test = grp.transform(test)\n",
    "\n",
    "# SRP\n",
    "srp = SparseRandomProjection(n_components=n_comp, dense_output=True, random_state=420)\n",
    "srp_results_train = srp.fit_transform(train.drop([\"y\"], axis=1))\n",
    "srp_results_test = srp.transform(test)\n",
    "\n",
    "#save columns list before adding the decomposition components\n",
    "\n",
    "\n",
    "# Append decomposition components to datasets\n",
    "for i in range(1, n_comp + 1):\n",
    "    train['pca_' + str(i)] = pca2_results_train[:, i - 1]\n",
    "    test['pca_' + str(i)] = pca2_results_test[:, i - 1]\n",
    "\n",
    "    train['ica_' + str(i)] = ica2_results_train[:, i - 1]\n",
    "    test['ica_' + str(i)] = ica2_results_test[:, i - 1]\n",
    "\n",
    "    train['tsvd_' + str(i)] = tsvd_results_train[:, i - 1]\n",
    "    test['tsvd_' + str(i)] = tsvd_results_test[:, i - 1]\n",
    "\n",
    "    train['grp_' + str(i)] = grp_results_train[:, i - 1]\n",
    "    test['grp_' + str(i)] = grp_results_test[:, i - 1]\n",
    "\n",
    "    train['srp_' + str(i)] = srp_results_train[:, i - 1]\n",
    "    test['srp_' + str(i)] = srp_results_test[:, i - 1]\n",
    "\n",
    "y_train = train['y'].values\n",
    "y_mean = np.mean(y_train)\n",
    "id_test = test['ID'].values\n",
    "\n",
    "finaltrainset = train[usable_columns]\n",
    "finaltestset = test[usable_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stacked_pipeline = make_pipeline(\n",
    "    StackingEstimator(estimator=LassoLarsCV(normalize=True)),\n",
    "    StackingEstimator(estimator=GradientBoostingRegressor(learning_rate=0.001, loss=\"huber\", max_depth=3, max_features=0.55, min_samples_leaf=18, min_samples_split=14, subsample=0.7)),\n",
    "    LassoLarsCV()\n",
    ")\n",
    "final = xgb.XGBRegressor(n_trees= 520,\n",
    "                eta=0.0045,\n",
    "                max_depth= 4,\n",
    "                subsample= 0.93,\n",
    "                objective= 'reg:linear',\n",
    "                eval_metric= 'rmse',\n",
    "                silent= 1)\n",
    "models =  [stacked_pipeline,\n",
    "            LinearRegression(),\n",
    "            RandomForestRegressor(),\n",
    "            LinearSVR(),\n",
    "           BayesianRidge(),\n",
    "           KNeighborsRegressor(n_neighbors=80,weights='uniform',p=2),\n",
    "           \n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ensemble(models, final_model, train_data, eval_data,  train_label, eval_label, xtest):\n",
    "    eval_preds = []\n",
    "    test_data = []\n",
    "    \n",
    "    \n",
    "    uc = ['ID', 'M9', 'M10', 'M11', 'X0', 'X5','X2', 'X314' ]\n",
    "    uc = []\n",
    "    \n",
    "#     for q in eval_data[uc].as_matrix():\n",
    "#     print(q.shape)\n",
    "#     print(eval_data['ID'])\n",
    "    for c in uc:\n",
    "        eval_preds.append(eval_data[c])\n",
    "        test_data.append(xtest[c])\n",
    "#     for q in xtest[uc]:\n",
    "#         test_data.append(q.as_matrix())\n",
    "    \n",
    "    for model in models:\n",
    "        model.fit(train_data, train_label)\n",
    "        eval_preds.append(model.predict(eval_data))\n",
    "        test_data.append(model.predict(xtest))\n",
    "        \n",
    "    eval_predsT = np.array(eval_preds).T\n",
    "    final_model.fit(eval_predsT, eval_label)\n",
    "    score = r2_score(eval_label, final_model.predict(eval_predsT))\n",
    "    test_dataT = np.array(test_data).T\n",
    "    \n",
    "    return final_model.predict(test_dataT), score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# r = []\n",
    "# mae = []\n",
    "# mse = []\n",
    "# stack_preds = []\n",
    "# from sklearn.svm import LinearSVR\n",
    "# for i in range(10):\n",
    "#     print(\"++===============================++\")\n",
    "#     train_X, eval_X, train_y, eval_y = train_test_split(finaltrainset, y_train, test_size=0.2 )\n",
    "#     cv_train_X, cv_eval_X, cv_train_y, cv_eval_y = train_test_split(train_X, train_y, test_size=0.6 )\n",
    "#     stack_pred, score = ensemble(models, final, cv_train_X, cv_eval_X, cv_train_y, cv_eval_y, eval_X)\n",
    "#     stack_preds.append(stack_pred)\n",
    "#     print(\"CV score : \", score)\n",
    "    \n",
    "#     r.append(score)\n",
    "#     mse.append(np.sqrt(mean_squared_error(eval_y, stack_pred)))\n",
    "#     mae.append(mean_absolute_error(eval_y, stack_pred))\n",
    "#     print(\"++===============================++\\n\")\n",
    "\n",
    "# print(\"r2 to probe : \", np.mean(r))\n",
    "# print(\"rmse  to probe : \", np.mean(mse))\n",
    "# print(\"mae  to probe: \", np.mean(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++===============================++\n",
      "CV score :  0.565174986296\n",
      "0 R2 score :  0.480260844461\n",
      "++===============================++\n",
      "\n",
      "++===============================++\n",
      "CV score :  0.564411762383\n",
      "1 R2 score :  0.481447914167\n",
      "++===============================++\n",
      "\n",
      "++===============================++\n",
      "CV score :  0.573368131814\n",
      "2 R2 score :  0.479152495273\n",
      "++===============================++\n",
      "\n",
      "++===============================++\n",
      "CV score :  0.57828345737\n",
      "3 R2 score :  0.481533332897\n",
      "++===============================++\n",
      "\n",
      "++===============================++\n",
      "CV score :  0.56897451332\n",
      "4 R2 score :  0.481126641459\n",
      "++===============================++\n",
      "\n",
      "++===============================++\n",
      "CV score :  0.575925507368\n",
      "5 R2 score :  0.480198221477\n",
      "++===============================++\n",
      "\n",
      "++===============================++\n",
      "CV score :  0.577940793167\n",
      "6 R2 score :  0.480701483334\n",
      "++===============================++\n",
      "\n",
      "++===============================++\n",
      "CV score :  0.564755665344\n",
      "7 R2 score :  0.48284404999\n",
      "++===============================++\n",
      "\n",
      "++===============================++\n",
      "CV score :  0.573150886686\n",
      "8 R2 score :  0.483503211965\n",
      "++===============================++\n",
      "\n",
      "++===============================++\n",
      "CV score :  0.57657811215\n",
      "9 R2 score :  0.478189316456\n",
      "++===============================++\n",
      "\n",
      "r2 to probe :  0.480895751148\n",
      "rmse  to probe :  10.1605295832\n",
      "mae  to probe:  6.99037455089\n"
     ]
    }
   ],
   "source": [
    "r = []\n",
    "mae = []\n",
    "mse = []\n",
    "stack_preds = []\n",
    "from sklearn.svm import LinearSVR\n",
    "for i in range(10):\n",
    "    print(\"++===============================++\")\n",
    "    train_X, eval_X, train_y, eval_y = train_test_split(finaltrainset, y_train, test_size=0.9 )\n",
    "    stack_pred, score = ensemble(models, final, train_X, eval_X, train_y, eval_y, finaltestset)\n",
    "    stack_preds.append(stack_pred)\n",
    "    print(\"CV score : \", score)\n",
    "    sub = pd.DataFrame()\n",
    "    sub['ID'] = id_test\n",
    "    sub['y'] = stack_pred #y_pred*0.75 + results*0.25\n",
    "    res = sub[sub.ID.isin(public_lb.id)]\n",
    "    print(i, \"R2 score : \", r2_score(public_lb.yValue, res.y))\n",
    "\n",
    "    r.append(r2_score(public_lb.yValue, res.y))\n",
    "    mse.append(np.sqrt(mean_squared_error(public_lb.yValue, res.y)))\n",
    "    mae.append(mean_absolute_error(public_lb.yValue, res.y))\n",
    "    print(\"++===============================++\\n\")\n",
    "\n",
    "print(\"r2 to probe : \", np.mean(r))\n",
    "print(\"rmse  to probe : \", np.mean(mse))\n",
    "print(\"mae  to probe: \", np.mean(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4209,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_predsT = np.array(stack_preds).T\n",
    "stack_predsTM = np.mean(stack_predsT,axis=1)\n",
    "stack_predsTM.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 to probe :  0.481048681301\n",
      "mse  to probe :  103.206164451\n",
      "mae  to probe:  6.98999765404\n"
     ]
    }
   ],
   "source": [
    "# y_pred = model.predict(dtest)\n",
    "# results = stacked_pipeline.predict(finaltestset)\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['ID'] = id_test\n",
    "sub['y'] = stack_predsTM#y_pred*0.75 + results*0.25\n",
    "res = sub[sub.ID.isin(public_lb.id)]\n",
    "result = r2_score(public_lb.yValue, res.y)\n",
    "print(\"r2 to probe : \", r2_score(public_lb.yValue, res.y))\n",
    "print(\"mse  to probe : \", mean_squared_error(public_lb.yValue, res.y))\n",
    "print(\"mae  to probe: \", mean_absolute_error(public_lb.yValue, res.y))\n",
    "exception = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base Results = 0.720429268928"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if probing:\n",
    "    first = sub[sub.ID.isin(public_lb.id) == False]\n",
    "    second = public_lb[['ID', 'y']]\n",
    "    sub2 = first.append(second, ignore_index=True)\n",
    "    sub2 = sub2.sort_values('ID', ascending=True)\n",
    "else:\n",
    "    sub2 = sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.969148391546\n"
     ]
    }
   ],
   "source": [
    "print(r2_score(best.y, sub2.y ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48068588507449228"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity check\n",
    "r2_score(public_lb.yValue, sub2[sub2.ID.isin(public_lb.id)].y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sanity_check= sub2[pd.isnull(sub2.y)].shape[0]\n",
    "print(sub2.shape)\n",
    "print(sanity_check == 0)\n",
    "print(sub2.shape[0] == test.shape[0])\n",
    "if sanity_check == 0 and sub2.shape[0] == test.shape[0] and (result + 0.001 > 0.720429268928 or exception):\n",
    "    sub2.to_csv('stacked-models7.csv', index=False)\n",
    "    print(\"saved\")\n",
    "else:\n",
    "    print(\"below can't continue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
