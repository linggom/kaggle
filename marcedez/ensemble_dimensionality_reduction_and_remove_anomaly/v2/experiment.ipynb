{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator,TransformerMixin, ClassifierMixin\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.linear_model import ElasticNetCV, LassoLarsCV, LinearRegression, HuberRegressor, RANSACRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StackingEstimator(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, estimator):\n",
    "        self.estimator = estimator\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        self.estimator.fit(X, y, **fit_params)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X = check_array(X)\n",
    "        X_transformed = np.copy(X)\n",
    "        # add class probabilities as a synthetic feature\n",
    "        if issubclass(self.estimator.__class__, ClassifierMixin) and hasattr(self.estimator, 'predict_proba'):\n",
    "            X_transformed = np.hstack((self.estimator.predict_proba(X), X))\n",
    "\n",
    "        # add class prodiction as a synthetic feature\n",
    "        X_transformed = np.hstack((np.reshape(self.estimator.predict(X), (-1, 1)), X_transformed))\n",
    "\n",
    "        return X_transformed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "train = pickle.load(open('train.pkl', 'rb'))\n",
    "train = pickle.load(open('test.pkl', 'rb'))\n",
    "# train = pd.read_csv('../../data/train.csv')\n",
    "original_train = pd.read_csv('../../data/train.csv')\n",
    "usable_columns = original_train.drop('y', axis=1).columns\n",
    "# test = pd.read_csv('../../data/test.csv')\n",
    "public_lb = pd.read_csv('../../data/public_lb.csv')\n",
    "public_lb['ID'] = public_lb.id\n",
    "public_lb['y'] = public_lb.yValue\n",
    "\n",
    "best = pd.read_csv('v8.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_probe = test[test.ID.isin(public_lb.id)]\n",
    "# test_probe = test_probe.insert(1, 'y', public_lb.y.as_matrix())\n",
    "# train = train.append(test_probe, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# usable_columns = usable_columns[:3].append(usable_columns[10:])\n",
    "# usable_columns=['ID', 'X0', 'X47','X95','X314','X315','X232','X119','X311','X76','X329','X238','X340','X362','X137']\n",
    "# usable_columns = ['X0', 'X5', 'X47', 'X2', 'X1', 'X8', 'X95', 'X6', 'X4', 'X240',\n",
    "#        'X104', 'X383', 'X203', 'X339', 'X292', 'X206', 'X196', 'X3', 'X84',\n",
    "#        'X342', 'X271', 'X207', 'X298', 'X163', 'X81', 'X91', 'X335',\n",
    "#        'X169', 'X77', 'X37', 'X68', 'X343', 'X291', 'X58', 'X78', 'X345',\n",
    "#        'X267', 'X160', 'X272', 'X226', 'X359', 'X305', 'X153', 'X191',\n",
    "#        'X275', 'X352', 'X70', 'X283', 'X231', 'X340', 'X327', 'X259',\n",
    "#        'X152', 'X375', 'X135', 'X165', 'X125', 'X274', 'X369', 'X382',\n",
    "#        'X16', 'X315', 'X295', 'X39', 'X341', 'X27', 'X75', 'X66', 'X151',\n",
    "#        'X277', 'X190', 'X338', 'X252', 'X73', 'X192', 'X24', 'X173',\n",
    "#        'X133', 'X363', 'X64', 'X23', 'X176', 'X148', 'X182', 'X258',\n",
    "#        'X286', 'X280', 'X228', 'X284', 'X332', 'X31', 'X45', 'X85', 'X96',\n",
    "#        'X22', 'X210', 'X65', 'X145', 'X21', 'X55']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for c in train.columns:\n",
    "#     if train[c].dtype == 'object':\n",
    "#         lbl = LabelEncoder()\n",
    "#         lbl.fit(list(train[c].values) + list(test[c].values))\n",
    "#         train[c] = lbl.transform(list(train[c].values))\n",
    "#         test[c] = lbl.transform(list(test[c].values))\n",
    "\n",
    "\n",
    "\n",
    "# n_comp = 12\n",
    "\n",
    "# # tSVD\n",
    "# tsvd = TruncatedSVD(n_components=n_comp, random_state=420)\n",
    "# tsvd_results_train = tsvd.fit_transform(train.drop([\"y\"], axis=1))\n",
    "# tsvd_results_test = tsvd.transform(test)\n",
    "\n",
    "# # PCA\n",
    "# pca = PCA(n_components=n_comp, random_state=420)\n",
    "# pca2_results_train = pca.fit_transform(train.drop([\"y\"], axis=1))\n",
    "# pca2_results_test = pca.transform(test)\n",
    "\n",
    "# # ICA\n",
    "# ica = FastICA(n_components=n_comp, random_state=420)\n",
    "# ica2_results_train = ica.fit_transform(train.drop([\"y\"], axis=1))\n",
    "# ica2_results_test = ica.transform(test)\n",
    "\n",
    "# # GRP\n",
    "# grp = GaussianRandomProjection(n_components=n_comp, eps=0.1, random_state=420)\n",
    "# grp_results_train = grp.fit_transform(train.drop([\"y\"], axis=1))\n",
    "# grp_results_test = grp.transform(test)\n",
    "\n",
    "# # SRP\n",
    "# srp = SparseRandomProjection(n_components=n_comp, dense_output=True, random_state=420)\n",
    "# srp_results_train = srp.fit_transform(train.drop([\"y\"], axis=1))\n",
    "# srp_results_test = srp.transform(test)\n",
    "\n",
    "# #save columns list before adding the decomposition components\n",
    "\n",
    "\n",
    "# # Append decomposition components to datasets\n",
    "# for i in range(1, n_comp + 1):\n",
    "#     train['pca_' + str(i)] = pca2_results_train[:, i - 1]\n",
    "#     test['pca_' + str(i)] = pca2_results_test[:, i - 1]\n",
    "\n",
    "#     train['ica_' + str(i)] = ica2_results_train[:, i - 1]\n",
    "#     test['ica_' + str(i)] = ica2_results_test[:, i - 1]\n",
    "\n",
    "#     train['tsvd_' + str(i)] = tsvd_results_train[:, i - 1]\n",
    "#     test['tsvd_' + str(i)] = tsvd_results_test[:, i - 1]\n",
    "\n",
    "#     train['grp_' + str(i)] = grp_results_train[:, i - 1]\n",
    "#     test['grp_' + str(i)] = grp_results_test[:, i - 1]\n",
    "\n",
    "#     train['srp_' + str(i)] = srp_results_train[:, i - 1]\n",
    "#     test['srp_' + str(i)] = srp_results_test[:, i - 1]\n",
    "\n",
    "# y_train = train['y'].values\n",
    "# y_mean = np.mean(y_train)\n",
    "# id_test = test['ID'].values\n",
    "# #finaltrainset and finaltestset are data to be used only the stacked model (does not contain PCA, SVD... arrays)\n",
    "# # usable_columns=['ID', 'X0', 'X47','X95','X314','X315','X232','X119','X311','X76','X329','X238','X340','X362','X137']\n",
    "\n",
    "# finaltrainset = train[usable_columns]\n",
    "# finaltestset = test[usable_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stacked_pipeline = make_pipeline(\n",
    "    StackingEstimator(estimator=LassoLarsCV(normalize=True)),\n",
    "    StackingEstimator(estimator=GradientBoostingRegressor(learning_rate=0.001, loss=\"huber\", max_depth=3, max_features=0.55, min_samples_leaf=18, min_samples_split=14, subsample=0.7)),\n",
    "    LassoLarsCV()\n",
    ")\n",
    "final = xgb.XGBRegressor(n_trees= 520,\n",
    "                eta=0.0045,\n",
    "                max_depth= 4,\n",
    "                subsample= 0.93,\n",
    "                objective= 'reg:linear',\n",
    "                eval_metric= 'rmse',\n",
    "                silent= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ensemble(models, final_model, train_data, eval_data,  train_label, eval_label, fulltest):\n",
    "    eval_preds = []\n",
    "    test_data = []\n",
    "    \n",
    "    for model in models:\n",
    "        model.fit(train_data, train_label)\n",
    "        eval_preds.append(model.predict(eval_data))\n",
    "        test_data.append(model.predict(fulltest))\n",
    "        \n",
    "    eval_predsT = np.array(eval_preds).T\n",
    "    final_model.fit(eval_predsT, eval_label)\n",
    "    score = r2_score(eval_label, final_model.predict(eval_predsT))\n",
    "    test_dataT = np.array(test_data).T\n",
    "    \n",
    "    return final_model.predict(test_dataT), score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++===============================++\n",
      "CV score :  0.620575673969\n",
      "0 0.229405127293\n",
      "++===============================++\n",
      "\n",
      "++===============================++\n",
      "CV score :  0.637503444888\n",
      "1 -0.279516055788\n",
      "++===============================++\n",
      "\n",
      "++===============================++\n",
      "CV score :  0.602689433153\n",
      "2 0.328495305476\n",
      "++===============================++\n",
      "\n",
      "++===============================++\n",
      "CV score :  0.603537077602\n",
      "3 0.328705340566\n",
      "++===============================++\n",
      "\n",
      "++===============================++\n",
      "CV score :  0.669766660218\n",
      "4 0.212532587472\n",
      "++===============================++\n",
      "\n",
      "++===============================++\n",
      "CV score :  0.646674249393\n",
      "5 0.143342084718\n",
      "++===============================++\n",
      "\n",
      "++===============================++\n",
      "CV score :  0.625244437428\n",
      "6 0.0341076581888\n",
      "++===============================++\n",
      "\n",
      "++===============================++\n",
      "CV score :  0.618219284966\n",
      "7 0.270560890689\n",
      "++===============================++\n",
      "\n",
      "++===============================++\n",
      "CV score :  0.616378383452\n",
      "8 0.205519167812\n",
      "++===============================++\n",
      "\n",
      "++===============================++\n",
      "CV score :  0.635703910893\n",
      "9 0.22776568451\n",
      "++===============================++\n",
      "\n",
      "r2 to probe :  0.170091779094\n",
      "rmse  to probe :  13.1858523755\n",
      "mae  to probe:  10.2383282788\n"
     ]
    }
   ],
   "source": [
    "r = []\n",
    "mae = []\n",
    "mse = []\n",
    "stack_preds = []\n",
    "from sklearn.svm import LinearSVR\n",
    "for i in range(10):\n",
    "    print(\"++===============================++\")\n",
    "    train_X,eval_X, train_y, eval_y = train_test_split(finaltrainset, y_train, test_size=0.9)\n",
    "    stack_pred, score = ensemble(\n",
    "                    [\n",
    "                        LassoLarsCV(normalize=True),\n",
    "                        LinearRegression(),\n",
    "                        RandomForestRegressor(),\n",
    "                        LinearSVR(),\n",
    "                        GradientBoostingRegressor(learning_rate=0.001, loss=\"huber\", max_depth=3, max_features=0.55, min_samples_leaf=18, min_samples_split=14, subsample=0.7)\n",
    "                   ],\n",
    "                    final,\n",
    "                   train_X,eval_X, train_y, eval_y,\n",
    "                   finaltestset.copy()            \n",
    "    )\n",
    "    stack_preds.append(stack_pred)\n",
    "    print(\"CV score : \", score)\n",
    "    sub = pd.DataFrame()\n",
    "    sub['ID'] = id_test\n",
    "    sub['y'] = stack_pred#y_pred*0.75 + results*0.25\n",
    "    res = sub[sub.ID.isin(public_lb.id)]\n",
    "    print(i, r2_score(public_lb.yValue, res.y))\n",
    "\n",
    "    r.append(r2_score(public_lb.yValue, res.y))\n",
    "    mse.append(np.sqrt(mean_squared_error(public_lb.yValue, res.y)))\n",
    "    mae.append(mean_absolute_error(public_lb.yValue, res.y))\n",
    "    print(\"++===============================++\\n\")\n",
    "\n",
    "print(\"r2 to probe : \", np.mean(r))\n",
    "print(\"rmse  to probe : \", np.mean(mse))\n",
    "print(\"mae  to probe: \", np.mean(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4209,)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_predsT = np.array(stack_preds).T\n",
    "stack_predsTM = np.mean(stack_predsT,axis=1)\n",
    "stack_predsTM.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 to probe :  0.271783067133\n",
      "mse  to probe :  153.986619445\n",
      "mae  to probe:  9.98537500008\n"
     ]
    }
   ],
   "source": [
    "# y_pred = model.predict(dtest)\n",
    "# results = stacked_pipeline.predict(finaltestset)\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['ID'] = id_test\n",
    "sub['y'] = stack_predsTM \n",
    "res = sub[sub.ID.isin(public_lb.id)]\n",
    "result = r2_score(public_lb.yValue, res.y)\n",
    "print(\"r2 to probe : \", r2_score(public_lb.yValue, res.y))\n",
    "print(\"mse  to probe : \", mean_squared_error(public_lb.yValue, res.y))\n",
    "print(\"mae  to probe: \", mean_absolute_error(public_lb.yValue, res.y))\n",
    "exception = True\n",
    "# res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base Results = 0.720429268928"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first = sub[sub.ID.isin(public_lb.id) == False]\n",
    "second = public_lb[['ID', 'y']]\n",
    "sub2 = first.append(second, ignore_index=True)\n",
    "sub2 = sub2.sort_values('ID', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 :  0.922348480499\n",
      "MSE :  2.63960146114\n",
      "MAE :  1.71631564195\n"
     ]
    }
   ],
   "source": [
    "print(\"R2 : \", r2_score(best.y, sub2.y ))\n",
    "print(\"MSE : \", np.sqrt(mean_squared_error(best.y, sub2.y )))\n",
    "print(\"MAE : \", mean_absolute_error(best.y, sub2.y ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity check\n",
    "r2_score(public_lb.yValue, sub2[sub2.ID.isin(public_lb.id)].y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4209, 2)\n",
      "True\n",
      "True\n",
      "saved\n"
     ]
    }
   ],
   "source": [
    "sanity_check= sub2[pd.isnull(sub2.y)].shape[0]\n",
    "print(sub2.shape)\n",
    "print(sanity_check == 0)\n",
    "print(sub2.shape[0] == test.shape[0])\n",
    "if sanity_check == 0 and sub2.shape[0] == test.shape[0] and (result + 0.001 > 0.720429268928 or exception):\n",
    "    sub2.to_csv('sm7.csv', index=False)\n",
    "    print(\"saved\")\n",
    "else:\n",
    "    print(\"below can't continue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
