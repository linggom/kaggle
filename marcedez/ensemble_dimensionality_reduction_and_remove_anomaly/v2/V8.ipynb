{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator,TransformerMixin, ClassifierMixin\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.linear_model import ElasticNetCV, LassoLarsCV, LinearRegression, HuberRegressor, RANSACRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StackingEstimator(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, estimator):\n",
    "        self.estimator = estimator\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        self.estimator.fit(X, y, **fit_params)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X = check_array(X)\n",
    "        X_transformed = np.copy(X)\n",
    "        # add class probabilities as a synthetic feature\n",
    "        if issubclass(self.estimator.__class__, ClassifierMixin) and hasattr(self.estimator, 'predict_proba'):\n",
    "            X_transformed = np.hstack((self.estimator.predict_proba(X), X))\n",
    "\n",
    "        # add class prodiction as a synthetic feature\n",
    "        X_transformed = np.hstack((np.reshape(self.estimator.predict(X), (-1, 1)), X_transformed))\n",
    "\n",
    "        return X_transformed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# train = pickle.load(open('train.pkl', 'rb'))\n",
    "train = pd.read_csv('../../data/train2.csv')\n",
    "original_train = pd.read_csv('../../data/train2.csv')\n",
    "usable_columns = original_train.drop('y', axis=1).columns\n",
    "test = pd.read_csv('../../data/test.csv')\n",
    "public_lb = pd.read_csv('../../data/public_lb.csv')\n",
    "public_lb['ID'] = public_lb.id\n",
    "public_lb['y'] = public_lb.yValue\n",
    "\n",
    "best = pd.read_csv('v7.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "usable_columns = usable_columns[:3].append(usable_columns[10:])\n",
    "# usable_columns=['ID', 'X0', 'X47','X95','X314','X315','X232','X119','X311','X76','X329','X238','X340','X362','X137']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for c in train.columns:\n",
    "    if train[c].dtype == 'object':\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(train[c].values) + list(test[c].values))\n",
    "        train[c] = lbl.transform(list(train[c].values))\n",
    "        test[c] = lbl.transform(list(test[c].values))\n",
    "\n",
    "\n",
    "\n",
    "n_comp = 12\n",
    "\n",
    "# tSVD\n",
    "tsvd = TruncatedSVD(n_components=n_comp, random_state=420)\n",
    "tsvd_results_train = tsvd.fit_transform(train.drop([\"y\"], axis=1))\n",
    "tsvd_results_test = tsvd.transform(test)\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=n_comp, random_state=420)\n",
    "pca2_results_train = pca.fit_transform(train.drop([\"y\"], axis=1))\n",
    "pca2_results_test = pca.transform(test)\n",
    "\n",
    "# ICA\n",
    "ica = FastICA(n_components=n_comp, random_state=420)\n",
    "ica2_results_train = ica.fit_transform(train.drop([\"y\"], axis=1))\n",
    "ica2_results_test = ica.transform(test)\n",
    "\n",
    "# GRP\n",
    "grp = GaussianRandomProjection(n_components=n_comp, eps=0.1, random_state=420)\n",
    "grp_results_train = grp.fit_transform(train.drop([\"y\"], axis=1))\n",
    "grp_results_test = grp.transform(test)\n",
    "\n",
    "# SRP\n",
    "srp = SparseRandomProjection(n_components=n_comp, dense_output=True, random_state=420)\n",
    "srp_results_train = srp.fit_transform(train.drop([\"y\"], axis=1))\n",
    "srp_results_test = srp.transform(test)\n",
    "\n",
    "#save columns list before adding the decomposition components\n",
    "\n",
    "\n",
    "# Append decomposition components to datasets\n",
    "for i in range(1, n_comp + 1):\n",
    "    train['pca_' + str(i)] = pca2_results_train[:, i - 1]\n",
    "    test['pca_' + str(i)] = pca2_results_test[:, i - 1]\n",
    "\n",
    "    train['ica_' + str(i)] = ica2_results_train[:, i - 1]\n",
    "    test['ica_' + str(i)] = ica2_results_test[:, i - 1]\n",
    "\n",
    "    train['tsvd_' + str(i)] = tsvd_results_train[:, i - 1]\n",
    "    test['tsvd_' + str(i)] = tsvd_results_test[:, i - 1]\n",
    "\n",
    "    train['grp_' + str(i)] = grp_results_train[:, i - 1]\n",
    "    test['grp_' + str(i)] = grp_results_test[:, i - 1]\n",
    "\n",
    "    train['srp_' + str(i)] = srp_results_train[:, i - 1]\n",
    "    test['srp_' + str(i)] = srp_results_test[:, i - 1]\n",
    "\n",
    "y_train = train['y'].values\n",
    "y_mean = np.mean(y_train)\n",
    "id_test = test['ID'].values\n",
    "#finaltrainset and finaltestset are data to be used only the stacked model (does not contain PCA, SVD... arrays)\n",
    "# usable_columns=['ID', 'X0', 'X47','X95','X314','X315','X232','X119','X311','X76','X329','X238','X340','X362','X137']\n",
    "\n",
    "finaltrainset = train[usable_columns]\n",
    "finaltestset = test[usable_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stacked_pipeline = make_pipeline(\n",
    "    StackingEstimator(estimator=LassoLarsCV(normalize=True)),\n",
    "    StackingEstimator(estimator=GradientBoostingRegressor(learning_rate=0.001, loss=\"huber\", max_depth=3, max_features=0.55, min_samples_leaf=18, min_samples_split=14, subsample=0.7)),\n",
    "    LassoLarsCV()\n",
    ")\n",
    "final = xgb.XGBRegressor(n_trees= 520,\n",
    "                eta=0.0045,\n",
    "                max_depth= 4,\n",
    "                subsample= 0.93,\n",
    "                objective= 'reg:linear',\n",
    "                eval_metric= 'rmse',\n",
    "                silent= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ensemble(models, final_model, train_data, eval_data,  train_label, eval_label, fulltest):\n",
    "    eval_preds = []\n",
    "    test_data = []\n",
    "    \n",
    "    for model in models:\n",
    "        model.fit(train_data, train_label)\n",
    "        eval_preds.append(model.predict(eval_data))\n",
    "        test_data.append(model.predict(fulltest))\n",
    "        \n",
    "    eval_predsT = np.array(eval_preds).T\n",
    "    final_model.fit(eval_predsT, eval_label)\n",
    "    score = r2_score(eval_label, final_model.predict(eval_predsT))\n",
    "    test_dataT = np.array(test_data).T\n",
    "    \n",
    "    return final_model.predict(test_dataT), score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++===============================++\n",
      "CV score :  0.681620204782\n",
      "0 0.622425490323\n",
      "++===============================++\n",
      "\n",
      "++===============================++\n",
      "CV score :  0.684203396979\n",
      "1 0.666477094477\n",
      "++===============================++\n",
      "\n",
      "++===============================++\n",
      "CV score :  0.688668376194\n",
      "2 0.415870295866\n",
      "++===============================++\n",
      "\n",
      "++===============================++\n",
      "CV score :  0.693893332341\n",
      "3 0.77472846134\n",
      "++===============================++\n",
      "\n",
      "++===============================++\n",
      "CV score :  0.691600424525\n",
      "4 0.688382871729\n",
      "++===============================++\n",
      "\n",
      "++===============================++\n",
      "CV score :  0.676025307738\n",
      "5 0.489509827894\n",
      "++===============================++\n",
      "\n",
      "++===============================++\n",
      "CV score :  0.672373491788\n",
      "6 0.642747237233\n",
      "++===============================++\n",
      "\n",
      "++===============================++\n",
      "CV score :  0.678742114114\n",
      "7 0.617552755914\n",
      "++===============================++\n",
      "\n",
      "++===============================++\n",
      "CV score :  0.677040620275\n",
      "8 0.689314196195\n",
      "++===============================++\n",
      "\n",
      "++===============================++\n",
      "CV score :  0.696074131526\n",
      "9 0.707225991189\n",
      "++===============================++\n",
      "\n",
      "r2 to probe :  0.631423422216\n",
      "rmse  to probe :  8.36017423996\n",
      "mae  to probe:  5.99367424726\n"
     ]
    }
   ],
   "source": [
    "r = []\n",
    "mae = []\n",
    "mse = []\n",
    "stack_preds = []\n",
    "from sklearn.svm import LinearSVR\n",
    "for i in range(10):\n",
    "    print(\"++===============================++\")\n",
    "    train_X,eval_X, train_y, eval_y = train_test_split(finaltrainset, y_train, test_size=0.9 )\n",
    "    stack_pred, score = ensemble(\n",
    "                    [\n",
    "                        LassoLarsCV(normalize=True),\n",
    "                        LassoLarsCV(),\n",
    "                        LinearRegression(),\n",
    "                        RandomForestRegressor(),\n",
    "                        LinearSVR(),\n",
    "                        GradientBoostingRegressor(learning_rate=0.001, loss=\"huber\", max_depth=3, max_features=0.55, min_samples_leaf=18, min_samples_split=14, subsample=0.7)\n",
    "                   ],\n",
    "                    final,\n",
    "                   train_X,eval_X, train_y, eval_y,\n",
    "                   finaltestset.copy()            \n",
    "    )\n",
    "    stack_preds.append(stack_pred)\n",
    "    print(\"CV score : \", score)\n",
    "    sub = pd.DataFrame()\n",
    "    sub['ID'] = id_test\n",
    "    sub['y'] = stack_pred#y_pred*0.75 + results*0.25\n",
    "    res = sub[sub.ID.isin(public_lb.id)]\n",
    "    print(i, r2_score(public_lb.yValue, res.y))\n",
    "\n",
    "    r.append(r2_score(public_lb.yValue, res.y))\n",
    "    mse.append(np.sqrt(mean_squared_error(public_lb.yValue, res.y)))\n",
    "    mae.append(mean_absolute_error(public_lb.yValue, res.y))\n",
    "    print(\"++===============================++\\n\")\n",
    "\n",
    "print(\"r2 to probe : \", np.mean(r))\n",
    "print(\"rmse  to probe : \", np.mean(mse))\n",
    "print(\"mae  to probe: \", np.mean(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4209,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_predsT = np.array(stack_preds).T\n",
    "stack_predsTM = np.mean(stack_predsT,axis=1)\n",
    "stack_predsTM.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 to probe :  0.726886586093\n",
      "mse  to probe :  52.7033235217\n",
      "mae  to probe:  5.6191492253\n"
     ]
    }
   ],
   "source": [
    "# y_pred = model.predict(dtest)\n",
    "# results = stacked_pipeline.predict(finaltestset)\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['ID'] = id_test\n",
    "sub['y'] = stack_predsTM#y_pred*0.75 + results*0.25\n",
    "res = sub[sub.ID.isin(public_lb.id)]\n",
    "result = r2_score(public_lb.yValue, res.y)\n",
    "print(\"r2 to probe : \", r2_score(public_lb.yValue, res.y))\n",
    "print(\"mse  to probe : \", mean_squared_error(public_lb.yValue, res.y))\n",
    "print(\"mae  to probe: \", mean_absolute_error(public_lb.yValue, res.y))\n",
    "exception = True\n",
    "# res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base Results = 0.720429268928"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first = sub[sub.ID.isin(public_lb.id) == False]\n",
    "second = public_lb[['ID', 'y']]\n",
    "sub2 = first.append(second, ignore_index=True)\n",
    "sub2 = sub2.sort_values('ID', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.992243380704\n"
     ]
    }
   ],
   "source": [
    "print(r2_score(best.y, sub2.y ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity check\n",
    "r2_score(public_lb.yValue, sub2[sub2.ID.isin(public_lb.id)].y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4209, 2)\n",
      "True\n",
      "True\n",
      "saved\n"
     ]
    }
   ],
   "source": [
    "sanity_check= sub2[pd.isnull(sub2.y)].shape[0]\n",
    "print(sub2.shape)\n",
    "print(sanity_check == 0)\n",
    "print(sub2.shape[0] == test.shape[0])\n",
    "if sanity_check == 0 and sub2.shape[0] == test.shape[0] and (result + 0.001 > 0.720429268928 or exception):\n",
    "    sub2.to_csv('v8.csv', index=False)\n",
    "    print(\"saved\")\n",
    "else:\n",
    "    print(\"below can't continue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
