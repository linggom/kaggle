{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Righnow candidate is : SVR with PCA n_components = 10 with results 0.23706564506631833"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.linear_model import LinearRegression, TheilSenRegressor,RANSACRegressor,HuberRegressor\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cellen=['X47','X95','X314','X315','X232','X119','X311','X76','X329','X238','X340','X362','X137','ID']\n",
    "t2 = train[cellen]\n",
    "test2 = test[cellen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X8</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X375</th>\n",
       "      <th>X376</th>\n",
       "      <th>X377</th>\n",
       "      <th>X378</th>\n",
       "      <th>X379</th>\n",
       "      <th>X380</th>\n",
       "      <th>X382</th>\n",
       "      <th>X383</th>\n",
       "      <th>X384</th>\n",
       "      <th>X385</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>az</td>\n",
       "      <td>v</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>d</td>\n",
       "      <td>t</td>\n",
       "      <td>a</td>\n",
       "      <td>w</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>t</td>\n",
       "      <td>b</td>\n",
       "      <td>ai</td>\n",
       "      <td>a</td>\n",
       "      <td>d</td>\n",
       "      <td>b</td>\n",
       "      <td>g</td>\n",
       "      <td>y</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>az</td>\n",
       "      <td>v</td>\n",
       "      <td>as</td>\n",
       "      <td>f</td>\n",
       "      <td>d</td>\n",
       "      <td>a</td>\n",
       "      <td>j</td>\n",
       "      <td>j</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>az</td>\n",
       "      <td>l</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>d</td>\n",
       "      <td>z</td>\n",
       "      <td>l</td>\n",
       "      <td>n</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>w</td>\n",
       "      <td>s</td>\n",
       "      <td>as</td>\n",
       "      <td>c</td>\n",
       "      <td>d</td>\n",
       "      <td>y</td>\n",
       "      <td>i</td>\n",
       "      <td>m</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 377 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  X0 X1  X2 X3 X4 X5 X6 X8  X10  ...   X375  X376  X377  X378  X379  \\\n",
       "0   1  az  v   n  f  d  t  a  w    0  ...      0     0     0     1     0   \n",
       "1   2   t  b  ai  a  d  b  g  y    0  ...      0     0     1     0     0   \n",
       "2   3  az  v  as  f  d  a  j  j    0  ...      0     0     0     1     0   \n",
       "3   4  az  l   n  f  d  z  l  n    0  ...      0     0     0     1     0   \n",
       "4   5   w  s  as  c  d  y  i  m    0  ...      1     0     0     0     0   \n",
       "\n",
       "   X380  X382  X383  X384  X385  \n",
       "0     0     0     0     0     0  \n",
       "1     0     0     0     0     0  \n",
       "2     0     0     0     0     0  \n",
       "3     0     0     0     0     0  \n",
       "4     0     0     0     0     0  \n",
       "\n",
       "[5 rows x 377 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels = train.iloc[:,1]\n",
    "train_feats = train.iloc[:, 2:]\n",
    "\n",
    "test_labels = test.iloc[:,1]\n",
    "test_feats = test.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_feats_encode = []\n",
    "train_feats_encode.append(train.iloc[:, 0])\n",
    "for i in range(train_feats.shape[1]):\n",
    "    arr = train_feats.iloc[:, i]\n",
    "    if arr.dtype == 'O':        \n",
    "        lblencod = LabelEncoder()\n",
    "        arr = lblencod.fit_transform(arr)\n",
    "    train_feats_encode.append(arr)\n",
    "train_feats_encode = np.array(train_feats_encode).T\n",
    "\n",
    "test_feats_encode = []\n",
    "test_feats_encode.append(test.iloc[:, 0])\n",
    "for i in range(test_feats.shape[1]):\n",
    "    arr = test_feats.iloc[:, i]\n",
    "    if arr.dtype == 'O':        \n",
    "        lblencod = LabelEncoder()\n",
    "        arr = lblencod.fit_transform(arr)\n",
    "    test_feats_encode.append(arr)\n",
    "test_feats_encode = np.array(test_feats_encode).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cellen=['X47','X95','X314','X315','X232','X119','X311','X76','X329','X238','X340','X362','X137', 'ID']\n",
    "train_feats_encode = train[cellen]\n",
    "test_feats_encode = test[cellen]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start to do the ensemble method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***XGBOOST***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***SVR***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Linear Regression***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** SGD Regressor ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Keras Deep Learning ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# define custom R2 metrics for Keras backend\n",
    "from keras import backend as K\n",
    "# to tune the NN\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD, Adam\n",
    "act_func = 'tanh'\n",
    "input_dims = train_feats_encode.shape[1]\n",
    "\n",
    "def r2_keras(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square( y_true - y_pred )) \n",
    "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) ) \n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# base model architecture definition\n",
    "def model():\n",
    "    model = Sequential()\n",
    "    #input layer\n",
    "    model.add(Dense(input_dims, input_dim=input_dims))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    # hidden layers\n",
    "    model.add(Dense(input_dims))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act_func))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Dense(input_dims//2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(act_func))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Dense(input_dims//4, activation=act_func))\n",
    "    \n",
    "    # output layer (y_pred)\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    \n",
    "    # compile this model\n",
    "    model.compile(loss='mean_squared_error', # one may use 'mean_absolute_error' as alternative\n",
    "                  optimizer='adam',\n",
    "                  metrics=[r2_keras] # you can add several if needed\n",
    "                 )\n",
    "    \n",
    "    # Visualize NN architecture\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = KerasRegressor(\n",
    "    build_fn=model, \n",
    "    nb_epoch=100, \n",
    "    batch_size=20,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Gradient Boosting Regressor **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X, eval_X, train_y, eval_y = train_test_split(train_feats_encode, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check(model):\n",
    "    pred_train = model.predict(train_X)\n",
    "    score_train = r2_score(train_y, pred_train)\n",
    "    pred_eval = model.predict(eval_X)\n",
    "    score_eval = r2_score(eval_y, pred_eval)\n",
    "    print(model.__class__.__name__)\n",
    "    print(\"r2 train = \", score_train)\n",
    "    print(\"r2 eval = \", score_eval)\n",
    "    print(\"====================================================\\n\")\n",
    "    return score_train, score_eval, pred_train, pred_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_model():\n",
    "    \n",
    "    model_theilsen = TheilSenRegressor(random_state=42)\n",
    "    model_theilsen.fit(train_X, train_y)\n",
    "    train_theilsen, test_theilsen, pred_train_theilsen, pred_eval_theilsen = check(model_theilsen)\n",
    "\n",
    "    model_ransac = RANSACRegressor(random_state=42)\n",
    "    model_ransac.fit(train_X, train_y)\n",
    "    train_ransac, test_ransac, pred_train_ransac, pred_eval_ransac = check(model_ransac)\n",
    "\n",
    "    model_huber = HuberRegressor()\n",
    "    model_huber.fit(train_X, train_y)\n",
    "    train_huber, test_huber, pred_train_huber, pred_eval_huber = check(model_huber)\n",
    "\n",
    "    \n",
    "    model_linear = LinearRegression()\n",
    "    model_linear.fit(train_X, train_y)\n",
    "    train_lr, test_lr, pred_train_lr, pred_eval_lr = check(model_linear)\n",
    "\n",
    "    model_sgd = SGDRegressor()\n",
    "    model_sgd.fit(train_X, train_y)\n",
    "    train_sgd, test_sgd, pred_train_sgd, pred_eval_sgd = check(model_sgd)\n",
    "        \n",
    "    model_xgb = XGBRegressor(seed = 0,\n",
    "      colsample_bytree = 0.7,\n",
    "      subsample = 0.9,\n",
    "      eta = 0.005,\n",
    "      max_depth = 4,\n",
    "      num_parallel_tree = 1,\n",
    "      min_child_weight = 1, objective='reg:linear', base_score=np.mean(train_labels))\n",
    "    model_xgb.fit(train_X, train_y)\n",
    "    train_xgb, test_xgb, pred_train_xgb, pred_eval_xgb = check(model_xgb)\n",
    "    \n",
    "    model_gbr = GradientBoostingRegressor()\n",
    "    model_gbr.fit(train_X, train_y)\n",
    "    train_gbr, test_gbr, pred_train_gbr, pred_eval_gbr = check(model_gbr)\n",
    "    \n",
    "    estimator.fit(\n",
    "        train_feats_encode, \n",
    "        train_labels, \n",
    "        epochs=300, # increase it to 20-100 to get better results\n",
    "        verbose=2,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    train_dl, test_dl, pred_train_dl, pred_eval_dl = check(estimator)\n",
    "    \n",
    "    columns=['lr', 'sgd', 'xgb', 'gbr', 'dl', 'thelisen', 'ransac', 'huber']\n",
    "   \n",
    "    test_res = (pred_eval_lr, pred_eval_sgd, pred_eval_xgb, pred_eval_gbr, pred_eval_dl, pred_eval_theilsen, pred_eval_ransac, pred_eval_huber)\n",
    "    \n",
    "    train_res = (pred_train_lr, pred_train_sgd, pred_train_xgb, pred_train_gbr, pred_train_dl, pred_train_theilsen, pred_train_ransac, pred_train_huber)\n",
    "    \n",
    "    train_res_pd = pd.DataFrame(data=np.column_stack(train_res),\n",
    "                  columns=columns)\n",
    "    \n",
    "    test_res_pd = pd.DataFrame(data=np.column_stack(test_res),\n",
    "                  columns=columns)\n",
    "    models = [model_linear, model_sgd, model_xgb, model_gbr, estimator, model_theilsen, model_ransac, model_huber]\n",
    "    return train_res_pd, test_res_pd, models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TheilSenRegressor\n",
      "r2 train =  0.551501871606\n",
      "r2 eval =  0.589070058014\n",
      "====================================================\n",
      "\n",
      "RANSACRegressor\n",
      "r2 train =  0.405778335308\n",
      "r2 eval =  0.386768234855\n",
      "====================================================\n",
      "\n",
      "HuberRegressor\n",
      "r2 train =  -0.51240767427\n",
      "r2 eval =  -0.519955200484\n",
      "====================================================\n",
      "\n",
      "LinearRegression\n",
      "r2 train =  0.569757178173\n",
      "r2 eval =  0.584261619989\n",
      "====================================================\n",
      "\n",
      "SGDRegressor\n",
      "r2 train =  -7.29096076001e+29\n",
      "r2 eval =  -7.25479525773e+29\n",
      "====================================================\n",
      "\n",
      "XGBRegressor\n",
      "r2 train =  0.679140752775\n",
      "r2 eval =  0.620746062347\n",
      "====================================================\n",
      "\n",
      "GradientBoostingRegressor\n",
      "r2 train =  0.614364247483\n",
      "r2 eval =  0.615899278966\n",
      "====================================================\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 377)               142506    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 377)               1508      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 377)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 377)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 377)               142506    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 377)               1508      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 377)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 377)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 188)               71064     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 188)               752       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 188)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 188)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 94)                17766     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 95        \n",
      "=================================================================\n",
      "Total params: 377,705\n",
      "Trainable params: 375,821\n",
      "Non-trainable params: 1,884\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/300\n",
      "3s - loss: 8006.7087 - r2_keras: -6.2123e+01\n",
      "Epoch 2/300\n",
      "2s - loss: 3163.8750 - r2_keras: -2.3444e+01\n",
      "Epoch 3/300\n",
      "2s - loss: 1485.7159 - r2_keras: -1.0222e+01\n",
      "Epoch 4/300\n",
      "2s - loss: 703.9865 - r2_keras: -4.1062e+00\n",
      "Epoch 5/300\n",
      "2s - loss: 355.8436 - r2_keras: -1.4863e+00\n",
      "Epoch 6/300\n",
      "2s - loss: 220.2893 - r2_keras: -4.8200e-01\n",
      "Epoch 7/300\n",
      "2s - loss: 175.9527 - r2_keras: -1.5636e-01\n",
      "Epoch 8/300\n",
      "2s - loss: 164.9101 - r2_keras: -8.3746e-02\n",
      "Epoch 9/300\n",
      "2s - loss: 161.5455 - r2_keras: -6.1297e-02\n",
      "Epoch 10/300\n",
      "2s - loss: 160.8894 - r2_keras: -5.6514e-02\n",
      "Epoch 11/300\n",
      "2s - loss: 160.7660 - r2_keras: -6.5866e-02\n",
      "Epoch 12/300\n",
      "2s - loss: 160.8051 - r2_keras: -6.6508e-02\n",
      "Epoch 13/300\n",
      "2s - loss: 160.3276 - r2_keras: -4.5136e-02\n",
      "Epoch 14/300\n",
      "2s - loss: 163.8555 - r2_keras: -9.5544e-02\n",
      "Epoch 15/300\n",
      "2s - loss: 159.7723 - r2_keras: -5.3432e-02\n",
      "Epoch 16/300\n",
      "2s - loss: 159.4125 - r2_keras: -5.6923e-02\n",
      "Epoch 17/300\n",
      "2s - loss: 159.4478 - r2_keras: -6.8527e-02\n",
      "Epoch 18/300\n",
      "2s - loss: 157.6512 - r2_keras: -3.5757e-02\n",
      "Epoch 19/300\n",
      "2s - loss: 151.3654 - r2_keras: 0.0070\n",
      "Epoch 20/300\n",
      "2s - loss: 142.5657 - r2_keras: 0.0674\n",
      "Epoch 21/300\n",
      "2s - loss: 132.9173 - r2_keras: 0.1534\n",
      "Epoch 22/300\n",
      "2s - loss: 119.8193 - r2_keras: 0.2306\n",
      "Epoch 23/300\n",
      "2s - loss: 111.3437 - r2_keras: 0.2846\n",
      "Epoch 24/300\n",
      "2s - loss: 107.0841 - r2_keras: 0.3314\n",
      "Epoch 25/300\n",
      "2s - loss: 106.8247 - r2_keras: 0.3138\n",
      "Epoch 26/300\n",
      "2s - loss: 100.1571 - r2_keras: 0.3600\n",
      "Epoch 27/300\n",
      "2s - loss: 97.9351 - r2_keras: 0.3811\n",
      "Epoch 28/300\n",
      "2s - loss: 95.9790 - r2_keras: 0.4053\n",
      "Epoch 29/300\n",
      "2s - loss: 94.8733 - r2_keras: 0.3947\n",
      "Epoch 30/300\n",
      "2s - loss: 92.0910 - r2_keras: 0.4224\n",
      "Epoch 31/300\n",
      "2s - loss: 91.5992 - r2_keras: 0.4298\n",
      "Epoch 32/300\n",
      "2s - loss: 94.1721 - r2_keras: 0.4046\n",
      "Epoch 33/300\n",
      "2s - loss: 89.6331 - r2_keras: 0.4489\n",
      "Epoch 34/300\n",
      "2s - loss: 90.2539 - r2_keras: 0.4432\n",
      "Epoch 35/300\n",
      "2s - loss: 93.0576 - r2_keras: 0.4210\n",
      "Epoch 36/300\n",
      "2s - loss: 88.3191 - r2_keras: 0.4533\n",
      "Epoch 37/300\n",
      "2s - loss: 92.3469 - r2_keras: 0.4221\n",
      "Epoch 38/300\n",
      "2s - loss: 89.6054 - r2_keras: 0.4298\n",
      "Epoch 39/300\n",
      "2s - loss: 89.3700 - r2_keras: 0.4431\n",
      "Epoch 40/300\n",
      "2s - loss: 86.3062 - r2_keras: 0.4649\n",
      "Epoch 41/300\n",
      "2s - loss: 86.1153 - r2_keras: 0.4593\n",
      "Epoch 42/300\n",
      "2s - loss: 86.5346 - r2_keras: 0.4517\n",
      "Epoch 43/300\n",
      "2s - loss: 85.4075 - r2_keras: 0.4590\n",
      "Epoch 44/300\n",
      "2s - loss: 82.1543 - r2_keras: 0.4890\n",
      "Epoch 45/300\n",
      "2s - loss: 82.8941 - r2_keras: 0.4906\n",
      "Epoch 46/300\n",
      "2s - loss: 82.1451 - r2_keras: 0.5005\n",
      "Epoch 47/300\n",
      "2s - loss: 81.2545 - r2_keras: 0.5094\n",
      "Epoch 48/300\n",
      "2s - loss: 83.9467 - r2_keras: 0.4857\n",
      "Epoch 49/300\n",
      "2s - loss: 81.4817 - r2_keras: 0.4857\n",
      "Epoch 50/300\n",
      "2s - loss: 82.6624 - r2_keras: 0.4935\n",
      "Epoch 51/300\n",
      "2s - loss: 79.6852 - r2_keras: 0.5165\n",
      "Epoch 52/300\n",
      "2s - loss: 84.0415 - r2_keras: 0.4801\n",
      "Epoch 53/300\n",
      "2s - loss: 83.3102 - r2_keras: 0.4806\n",
      "Epoch 54/300\n",
      "2s - loss: 81.3991 - r2_keras: 0.4970\n",
      "Epoch 55/300\n",
      "2s - loss: 80.6389 - r2_keras: 0.5133\n",
      "Epoch 56/300\n",
      "2s - loss: 78.2773 - r2_keras: 0.5294\n",
      "Epoch 57/300\n",
      "2s - loss: 79.3615 - r2_keras: 0.5210\n",
      "Epoch 58/300\n",
      "2s - loss: 82.2953 - r2_keras: 0.4901\n",
      "Epoch 59/300\n",
      "2s - loss: 80.3792 - r2_keras: 0.5127\n",
      "Epoch 60/300\n",
      "2s - loss: 80.2800 - r2_keras: 0.4950\n",
      "Epoch 61/300\n",
      "2s - loss: 79.6158 - r2_keras: 0.5077\n",
      "Epoch 62/300\n",
      "2s - loss: 77.7800 - r2_keras: 0.5252\n",
      "Epoch 63/300\n",
      "2s - loss: 79.7638 - r2_keras: 0.5144\n",
      "Epoch 64/300\n",
      "2s - loss: 78.4580 - r2_keras: 0.5278\n",
      "Epoch 65/300\n",
      "2s - loss: 79.2653 - r2_keras: 0.5108\n",
      "Epoch 66/300\n",
      "2s - loss: 79.8085 - r2_keras: 0.5057\n",
      "Epoch 67/300\n",
      "2s - loss: 79.4659 - r2_keras: 0.5058\n",
      "Epoch 68/300\n",
      "2s - loss: 78.9762 - r2_keras: 0.5125\n",
      "Epoch 69/300\n",
      "2s - loss: 76.7871 - r2_keras: 0.5340\n",
      "Epoch 70/300\n",
      "2s - loss: 76.9522 - r2_keras: 0.5238\n",
      "Epoch 71/300\n",
      "2s - loss: 76.7519 - r2_keras: 0.5406\n",
      "Epoch 72/300\n",
      "2s - loss: 78.7605 - r2_keras: 0.5096\n",
      "Epoch 73/300\n",
      "2s - loss: 77.2572 - r2_keras: 0.5345\n",
      "Epoch 74/300\n",
      "2s - loss: 77.4384 - r2_keras: 0.5339\n",
      "Epoch 75/300\n",
      "2s - loss: 78.6921 - r2_keras: 0.5235\n",
      "Epoch 76/300\n",
      "2s - loss: 79.3079 - r2_keras: 0.5080\n",
      "Epoch 77/300\n",
      "2s - loss: 78.3614 - r2_keras: 0.5121\n",
      "Epoch 78/300\n",
      "2s - loss: 77.7434 - r2_keras: 0.5246\n",
      "Epoch 79/300\n",
      "2s - loss: 75.4180 - r2_keras: 0.5366\n",
      "Epoch 80/300\n",
      "2s - loss: 76.3731 - r2_keras: 0.5364\n",
      "Epoch 81/300\n",
      "2s - loss: 75.9893 - r2_keras: 0.5485\n",
      "Epoch 82/300\n",
      "2s - loss: 77.8294 - r2_keras: 0.5335\n",
      "Epoch 83/300\n",
      "2s - loss: 76.9108 - r2_keras: 0.5249\n",
      "Epoch 84/300\n",
      "2s - loss: 76.0103 - r2_keras: 0.5497\n",
      "Epoch 85/300\n",
      "2s - loss: 77.1003 - r2_keras: 0.5228\n",
      "Epoch 86/300\n",
      "2s - loss: 74.9143 - r2_keras: 0.5453\n",
      "Epoch 87/300\n",
      "2s - loss: 75.3452 - r2_keras: 0.5379\n",
      "Epoch 88/300\n",
      "2s - loss: 74.6000 - r2_keras: 0.5635\n",
      "Epoch 89/300\n",
      "2s - loss: 76.1805 - r2_keras: 0.5392\n",
      "Epoch 90/300\n",
      "2s - loss: 75.7603 - r2_keras: 0.5441\n",
      "Epoch 91/300\n",
      "2s - loss: 75.6823 - r2_keras: 0.5430\n",
      "Epoch 92/300\n",
      "2s - loss: 77.4327 - r2_keras: 0.5313\n",
      "Epoch 93/300\n",
      "2s - loss: 77.3676 - r2_keras: 0.5223\n",
      "Epoch 94/300\n",
      "2s - loss: 76.7842 - r2_keras: 0.5282\n",
      "Epoch 95/300\n",
      "2s - loss: 78.5366 - r2_keras: 0.5218\n",
      "Epoch 96/300\n",
      "2s - loss: 76.4148 - r2_keras: 0.5378\n",
      "Epoch 97/300\n",
      "2s - loss: 79.0730 - r2_keras: 0.5237\n",
      "Epoch 98/300\n",
      "2s - loss: 74.7905 - r2_keras: 0.5564\n",
      "Epoch 99/300\n",
      "2s - loss: 75.7850 - r2_keras: 0.5433\n",
      "Epoch 100/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2s - loss: 75.2540 - r2_keras: 0.5453\n",
      "Epoch 101/300\n",
      "2s - loss: 76.3963 - r2_keras: 0.5347\n",
      "Epoch 102/300\n",
      "2s - loss: 75.3213 - r2_keras: 0.5416\n",
      "Epoch 103/300\n",
      "2s - loss: 75.8720 - r2_keras: 0.5399\n",
      "Epoch 104/300\n",
      "2s - loss: 74.7893 - r2_keras: 0.5523\n",
      "Epoch 105/300\n",
      "2s - loss: 73.8358 - r2_keras: 0.5611\n",
      "Epoch 106/300\n",
      "2s - loss: 74.7656 - r2_keras: 0.5363\n",
      "Epoch 107/300\n",
      "2s - loss: 75.2843 - r2_keras: 0.5389\n",
      "Epoch 108/300\n",
      "2s - loss: 74.6267 - r2_keras: 0.5582\n",
      "Epoch 109/300\n",
      "2s - loss: 75.1551 - r2_keras: 0.5461\n",
      "Epoch 110/300\n",
      "2s - loss: 74.8582 - r2_keras: 0.5407\n",
      "Epoch 111/300\n",
      "2s - loss: 74.5446 - r2_keras: 0.5510\n",
      "Epoch 112/300\n",
      "2s - loss: 74.5478 - r2_keras: 0.5531\n",
      "Epoch 113/300\n",
      "2s - loss: 73.8505 - r2_keras: 0.5585\n",
      "Epoch 114/300\n",
      "2s - loss: 74.7175 - r2_keras: 0.5493\n",
      "Epoch 115/300\n",
      "2s - loss: 75.8651 - r2_keras: 0.5298\n",
      "Epoch 116/300\n",
      "2s - loss: 74.8424 - r2_keras: 0.5430\n",
      "Epoch 117/300\n",
      "2s - loss: 74.1282 - r2_keras: 0.5529\n",
      "Epoch 118/300\n",
      "2s - loss: 73.1013 - r2_keras: 0.5567\n",
      "Epoch 119/300\n",
      "2s - loss: 73.0144 - r2_keras: 0.5713\n",
      "Epoch 120/300\n",
      "2s - loss: 75.1785 - r2_keras: 0.5456\n",
      "Epoch 121/300\n",
      "2s - loss: 73.9094 - r2_keras: 0.5524\n",
      "Epoch 122/300\n",
      "2s - loss: 73.9450 - r2_keras: 0.5638\n",
      "Epoch 123/300\n",
      "2s - loss: 74.0057 - r2_keras: 0.5645\n",
      "Epoch 124/300\n",
      "2s - loss: 75.1824 - r2_keras: 0.5554\n",
      "Epoch 125/300\n",
      "2s - loss: 74.7847 - r2_keras: 0.5412\n",
      "Epoch 126/300\n",
      "2s - loss: 75.5219 - r2_keras: 0.5380\n",
      "Epoch 127/300\n",
      "2s - loss: 76.3137 - r2_keras: 0.5315\n",
      "Epoch 128/300\n",
      "2s - loss: 75.9225 - r2_keras: 0.5427\n",
      "Epoch 129/300\n",
      "2s - loss: 75.2240 - r2_keras: 0.5413\n",
      "Epoch 130/300\n",
      "2s - loss: 75.5988 - r2_keras: 0.5431\n",
      "Epoch 131/300\n",
      "2s - loss: 75.3750 - r2_keras: 0.5461\n",
      "Epoch 132/300\n",
      "2s - loss: 76.0587 - r2_keras: 0.5453\n",
      "Epoch 133/300\n",
      "2s - loss: 75.7160 - r2_keras: 0.5394\n",
      "Epoch 134/300\n",
      "2s - loss: 74.4733 - r2_keras: 0.5480\n",
      "Epoch 135/300\n",
      "2s - loss: 75.0543 - r2_keras: 0.5507\n",
      "Epoch 136/300\n",
      "2s - loss: 72.6885 - r2_keras: 0.5702\n",
      "Epoch 137/300\n",
      "2s - loss: 73.9187 - r2_keras: 0.5519\n",
      "Epoch 138/300\n",
      "2s - loss: 74.1724 - r2_keras: 0.5501\n",
      "Epoch 139/300\n",
      "2s - loss: 75.4425 - r2_keras: 0.5456\n",
      "Epoch 140/300\n",
      "2s - loss: 73.6365 - r2_keras: 0.5555\n",
      "Epoch 141/300\n",
      "2s - loss: 74.0358 - r2_keras: 0.5550\n",
      "Epoch 142/300\n",
      "2s - loss: 73.8209 - r2_keras: 0.5554\n",
      "Epoch 143/300\n",
      "2s - loss: 73.0421 - r2_keras: 0.5604\n",
      "Epoch 144/300\n",
      "2s - loss: 73.2657 - r2_keras: 0.5618\n",
      "Epoch 145/300\n",
      "2s - loss: 73.2937 - r2_keras: 0.5695\n",
      "Epoch 146/300\n",
      "2s - loss: 73.5018 - r2_keras: 0.5602\n",
      "Epoch 147/300\n",
      "2s - loss: 73.8677 - r2_keras: 0.5672\n",
      "Epoch 148/300\n",
      "2s - loss: 71.8933 - r2_keras: 0.5703\n",
      "Epoch 149/300\n",
      "2s - loss: 73.1735 - r2_keras: 0.5575\n",
      "Epoch 150/300\n",
      "2s - loss: 73.4627 - r2_keras: 0.5520\n",
      "Epoch 151/300\n",
      "2s - loss: 72.8522 - r2_keras: 0.5652\n",
      "Epoch 152/300\n",
      "2s - loss: 72.4141 - r2_keras: 0.5679\n",
      "Epoch 153/300\n",
      "2s - loss: 72.8212 - r2_keras: 0.5571\n",
      "Epoch 154/300\n",
      "2s - loss: 72.1960 - r2_keras: 0.5644\n",
      "Epoch 155/300\n",
      "2s - loss: 72.4890 - r2_keras: 0.5664\n",
      "Epoch 156/300\n",
      "2s - loss: 74.0589 - r2_keras: 0.5362\n",
      "Epoch 157/300\n",
      "2s - loss: 74.4289 - r2_keras: 0.5459\n",
      "Epoch 158/300\n",
      "2s - loss: 73.3762 - r2_keras: 0.5573\n",
      "Epoch 159/300\n",
      "2s - loss: 72.5798 - r2_keras: 0.5715\n",
      "Epoch 160/300\n",
      "2s - loss: 72.6436 - r2_keras: 0.5646\n",
      "Epoch 161/300\n",
      "2s - loss: 73.5328 - r2_keras: 0.5577\n",
      "Epoch 162/300\n",
      "2s - loss: 71.5013 - r2_keras: 0.5733\n",
      "Epoch 163/300\n",
      "2s - loss: 72.1039 - r2_keras: 0.5685\n",
      "Epoch 164/300\n",
      "2s - loss: 72.2416 - r2_keras: 0.5631\n",
      "Epoch 165/300\n",
      "2s - loss: 73.9714 - r2_keras: 0.5449\n",
      "Epoch 166/300\n",
      "2s - loss: 72.6997 - r2_keras: 0.5612\n",
      "Epoch 167/300\n",
      "2s - loss: 72.8816 - r2_keras: 0.5630\n",
      "Epoch 168/300\n",
      "2s - loss: 72.7918 - r2_keras: 0.5648\n",
      "Epoch 169/300\n",
      "2s - loss: 73.3202 - r2_keras: 0.5586\n",
      "Epoch 170/300\n",
      "2s - loss: 72.1433 - r2_keras: 0.5683\n",
      "Epoch 171/300\n",
      "2s - loss: 73.0197 - r2_keras: 0.5616\n",
      "Epoch 172/300\n",
      "2s - loss: 71.2420 - r2_keras: 0.5738\n",
      "Epoch 173/300\n",
      "2s - loss: 72.4119 - r2_keras: 0.5662\n",
      "Epoch 174/300\n",
      "2s - loss: 71.2030 - r2_keras: 0.5693\n",
      "Epoch 175/300\n",
      "2s - loss: 72.7891 - r2_keras: 0.5504\n",
      "Epoch 176/300\n",
      "2s - loss: 72.2992 - r2_keras: 0.5525\n",
      "Epoch 177/300\n",
      "2s - loss: 71.6143 - r2_keras: 0.5707\n",
      "Epoch 178/300\n",
      "2s - loss: 71.6647 - r2_keras: 0.5788\n",
      "Epoch 179/300\n",
      "2s - loss: 72.0841 - r2_keras: 0.5693\n",
      "Epoch 180/300\n",
      "2s - loss: 71.2293 - r2_keras: 0.5706\n",
      "Epoch 181/300\n",
      "2s - loss: 71.8295 - r2_keras: 0.5763\n",
      "Epoch 182/300\n",
      "2s - loss: 73.5454 - r2_keras: 0.5530\n",
      "Epoch 183/300\n",
      "2s - loss: 73.5244 - r2_keras: 0.5542\n",
      "Epoch 184/300\n",
      "2s - loss: 72.6020 - r2_keras: 0.5654\n",
      "Epoch 185/300\n",
      "2s - loss: 73.4180 - r2_keras: 0.5586\n",
      "Epoch 186/300\n",
      "2s - loss: 72.9231 - r2_keras: 0.5526\n",
      "Epoch 187/300\n",
      "2s - loss: 72.3661 - r2_keras: 0.5619\n",
      "Epoch 188/300\n",
      "2s - loss: 72.1058 - r2_keras: 0.5641\n",
      "Epoch 189/300\n",
      "2s - loss: 71.1962 - r2_keras: 0.5746\n",
      "Epoch 190/300\n",
      "2s - loss: 72.2521 - r2_keras: 0.5748\n",
      "Epoch 191/300\n",
      "2s - loss: 72.9042 - r2_keras: 0.5544\n",
      "Epoch 192/300\n",
      "2s - loss: 72.2593 - r2_keras: 0.5723\n",
      "Epoch 193/300\n",
      "2s - loss: 72.2484 - r2_keras: 0.5717\n",
      "Epoch 194/300\n",
      "2s - loss: 73.8146 - r2_keras: 0.5552\n",
      "Epoch 195/300\n",
      "2s - loss: 72.5999 - r2_keras: 0.5612\n",
      "Epoch 196/300\n",
      "2s - loss: 72.1670 - r2_keras: 0.5612\n",
      "Epoch 197/300\n",
      "2s - loss: 71.2802 - r2_keras: 0.5779\n",
      "Epoch 198/300\n",
      "2s - loss: 71.4091 - r2_keras: 0.5735\n",
      "Epoch 199/300\n",
      "2s - loss: 71.3750 - r2_keras: 0.5778\n",
      "Epoch 200/300\n",
      "2s - loss: 71.1965 - r2_keras: 0.5734\n",
      "Epoch 201/300\n",
      "2s - loss: 72.9933 - r2_keras: 0.5451\n",
      "Epoch 202/300\n",
      "2s - loss: 71.8789 - r2_keras: 0.5732\n",
      "Epoch 203/300\n",
      "2s - loss: 71.9647 - r2_keras: 0.5653\n",
      "Epoch 204/300\n",
      "2s - loss: 71.9844 - r2_keras: 0.5764\n",
      "Epoch 205/300\n",
      "2s - loss: 72.5927 - r2_keras: 0.5610\n",
      "Epoch 206/300\n",
      "2s - loss: 71.4324 - r2_keras: 0.5759\n",
      "Epoch 207/300\n",
      "2s - loss: 71.7923 - r2_keras: 0.5726\n",
      "Epoch 208/300\n",
      "2s - loss: 71.7103 - r2_keras: 0.5644\n",
      "Epoch 209/300\n",
      "2s - loss: 72.3069 - r2_keras: 0.5621\n",
      "Epoch 210/300\n",
      "2s - loss: 70.2053 - r2_keras: 0.5850\n",
      "Epoch 211/300\n",
      "2s - loss: 71.4091 - r2_keras: 0.5718\n",
      "Epoch 212/300\n",
      "2s - loss: 71.0876 - r2_keras: 0.5572\n",
      "Epoch 213/300\n",
      "2s - loss: 71.9212 - r2_keras: 0.5674\n",
      "Epoch 214/300\n",
      "2s - loss: 72.9129 - r2_keras: 0.5636\n",
      "Epoch 215/300\n",
      "2s - loss: 71.7868 - r2_keras: 0.5832\n",
      "Epoch 216/300\n",
      "2s - loss: 73.0120 - r2_keras: 0.5537\n",
      "Epoch 217/300\n",
      "2s - loss: 71.3732 - r2_keras: 0.5794\n",
      "Epoch 218/300\n",
      "2s - loss: 70.1207 - r2_keras: 0.5871\n",
      "Epoch 219/300\n",
      "2s - loss: 71.6941 - r2_keras: 0.5711\n",
      "Epoch 220/300\n",
      "2s - loss: 73.5661 - r2_keras: 0.5588\n",
      "Epoch 221/300\n",
      "2s - loss: 72.2851 - r2_keras: 0.5634\n",
      "Epoch 222/300\n",
      "2s - loss: 72.1702 - r2_keras: 0.5770\n",
      "Epoch 223/300\n",
      "2s - loss: 71.6635 - r2_keras: 0.5629\n",
      "Epoch 224/300\n",
      "2s - loss: 71.6067 - r2_keras: 0.5765\n",
      "Epoch 225/300\n",
      "2s - loss: 71.3982 - r2_keras: 0.5719\n",
      "Epoch 226/300\n",
      "2s - loss: 70.7718 - r2_keras: 0.5736\n",
      "Epoch 227/300\n",
      "2s - loss: 70.8941 - r2_keras: 0.5761\n",
      "Epoch 228/300\n",
      "2s - loss: 71.5807 - r2_keras: 0.5778\n",
      "Epoch 229/300\n",
      "2s - loss: 71.7096 - r2_keras: 0.5734\n",
      "Epoch 230/300\n",
      "2s - loss: 70.2279 - r2_keras: 0.5827\n",
      "Epoch 231/300\n",
      "2s - loss: 72.4339 - r2_keras: 0.5615\n",
      "Epoch 232/300\n",
      "2s - loss: 72.2819 - r2_keras: 0.5658\n",
      "Epoch 233/300\n",
      "2s - loss: 70.7440 - r2_keras: 0.5803\n",
      "Epoch 234/300\n",
      "2s - loss: 74.9950 - r2_keras: 0.5462\n",
      "Epoch 235/300\n",
      "2s - loss: 72.0056 - r2_keras: 0.5662\n",
      "Epoch 236/300\n",
      "2s - loss: 72.4745 - r2_keras: 0.5639\n",
      "Epoch 237/300\n",
      "2s - loss: 71.3167 - r2_keras: 0.5725\n",
      "Epoch 238/300\n",
      "2s - loss: 72.2555 - r2_keras: 0.5694\n",
      "Epoch 239/300\n",
      "2s - loss: 71.8860 - r2_keras: 0.5724\n",
      "Epoch 240/300\n",
      "2s - loss: 72.2557 - r2_keras: 0.5698\n",
      "Epoch 241/300\n",
      "2s - loss: 70.9885 - r2_keras: 0.5770\n",
      "Epoch 242/300\n",
      "2s - loss: 71.5714 - r2_keras: 0.5787\n",
      "Epoch 243/300\n",
      "2s - loss: 71.6084 - r2_keras: 0.5759\n",
      "Epoch 244/300\n",
      "2s - loss: 71.0157 - r2_keras: 0.5726\n",
      "Epoch 245/300\n",
      "2s - loss: 72.2356 - r2_keras: 0.5545\n",
      "Epoch 246/300\n",
      "2s - loss: 70.9480 - r2_keras: 0.5713\n",
      "Epoch 247/300\n",
      "2s - loss: 70.5774 - r2_keras: 0.5754\n",
      "Epoch 248/300\n",
      "2s - loss: 70.9790 - r2_keras: 0.5702\n",
      "Epoch 249/300\n",
      "2s - loss: 71.6874 - r2_keras: 0.5712\n",
      "Epoch 250/300\n",
      "2s - loss: 72.1753 - r2_keras: 0.5674\n",
      "Epoch 251/300\n",
      "2s - loss: 69.5125 - r2_keras: 0.5815\n",
      "Epoch 252/300\n",
      "2s - loss: 70.1630 - r2_keras: 0.5819\n",
      "Epoch 253/300\n",
      "2s - loss: 72.8737 - r2_keras: 0.5602\n",
      "Epoch 254/300\n",
      "2s - loss: 71.0756 - r2_keras: 0.5661\n",
      "Epoch 255/300\n",
      "2s - loss: 71.8509 - r2_keras: 0.5618\n",
      "Epoch 256/300\n",
      "2s - loss: 71.1088 - r2_keras: 0.5694\n",
      "Epoch 257/300\n",
      "2s - loss: 72.5128 - r2_keras: 0.5629\n",
      "Epoch 258/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2s - loss: 71.2423 - r2_keras: 0.5698\n",
      "Epoch 259/300\n",
      "2s - loss: 71.3027 - r2_keras: 0.5702\n",
      "Epoch 260/300\n",
      "2s - loss: 71.6712 - r2_keras: 0.5797\n",
      "Epoch 261/300\n",
      "2s - loss: 70.9812 - r2_keras: 0.5738\n",
      "Epoch 262/300\n",
      "2s - loss: 71.8624 - r2_keras: 0.5669\n",
      "Epoch 263/300\n",
      "2s - loss: 70.3517 - r2_keras: 0.5841\n",
      "Epoch 264/300\n",
      "2s - loss: 72.2504 - r2_keras: 0.5487\n",
      "Epoch 265/300\n",
      "2s - loss: 70.9702 - r2_keras: 0.5699\n",
      "Epoch 266/300\n",
      "2s - loss: 71.1205 - r2_keras: 0.5772\n",
      "Epoch 267/300\n",
      "2s - loss: 71.1460 - r2_keras: 0.5829\n",
      "Epoch 268/300\n",
      "2s - loss: 71.0772 - r2_keras: 0.5746\n",
      "Epoch 269/300\n",
      "2s - loss: 71.2702 - r2_keras: 0.5792\n",
      "Epoch 270/300\n",
      "2s - loss: 71.3162 - r2_keras: 0.5770\n",
      "Epoch 271/300\n",
      "2s - loss: 71.0594 - r2_keras: 0.5721\n",
      "Epoch 272/300\n",
      "2s - loss: 71.0867 - r2_keras: 0.5821\n",
      "Epoch 273/300\n",
      "2s - loss: 70.5222 - r2_keras: 0.5775\n",
      "Epoch 274/300\n",
      "2s - loss: 69.5829 - r2_keras: 0.5767\n",
      "Epoch 275/300\n",
      "2s - loss: 70.9564 - r2_keras: 0.5669\n",
      "Epoch 276/300\n",
      "2s - loss: 71.6926 - r2_keras: 0.5780\n",
      "Epoch 277/300\n",
      "2s - loss: 71.7150 - r2_keras: 0.5721\n",
      "Epoch 278/300\n",
      "2s - loss: 72.9806 - r2_keras: 0.5738\n",
      "Epoch 279/300\n",
      "2s - loss: 70.4596 - r2_keras: 0.5793\n",
      "Epoch 280/300\n",
      "2s - loss: 71.0737 - r2_keras: 0.5779\n",
      "Epoch 281/300\n",
      "2s - loss: 70.0664 - r2_keras: 0.5981\n",
      "Epoch 282/300\n",
      "2s - loss: 72.3951 - r2_keras: 0.5640\n",
      "Epoch 283/300\n",
      "2s - loss: 72.3085 - r2_keras: 0.5675\n",
      "Epoch 284/300\n",
      "2s - loss: 71.0935 - r2_keras: 0.5797\n",
      "Epoch 285/300\n",
      "2s - loss: 70.0326 - r2_keras: 0.5883\n",
      "Epoch 286/300\n",
      "2s - loss: 70.8757 - r2_keras: 0.5795\n",
      "Epoch 287/300\n",
      "2s - loss: 70.7525 - r2_keras: 0.5748\n",
      "Epoch 288/300\n",
      "2s - loss: 70.5954 - r2_keras: 0.5842\n",
      "Epoch 289/300\n",
      "2s - loss: 70.9395 - r2_keras: 0.5769\n",
      "Epoch 290/300\n",
      "2s - loss: 71.5751 - r2_keras: 0.5743\n",
      "Epoch 291/300\n",
      "2s - loss: 71.3018 - r2_keras: 0.5732\n",
      "Epoch 292/300\n",
      "2s - loss: 71.7981 - r2_keras: 0.5730\n",
      "Epoch 293/300\n",
      "2s - loss: 70.4603 - r2_keras: 0.5783\n",
      "Epoch 294/300\n",
      "2s - loss: 70.3784 - r2_keras: 0.5888\n",
      "Epoch 295/300\n",
      "2s - loss: 70.6985 - r2_keras: 0.5728\n",
      "Epoch 296/300\n",
      "2s - loss: 71.3930 - r2_keras: 0.5714\n",
      "Epoch 297/300\n",
      "2s - loss: 71.5105 - r2_keras: 0.5774\n",
      "Epoch 298/300\n",
      "2s - loss: 72.7350 - r2_keras: 0.5679\n",
      "Epoch 299/300\n",
      "2s - loss: 70.7460 - r2_keras: 0.5751\n",
      "Epoch 300/300\n",
      "2s - loss: 72.1515 - r2_keras: 0.5678\n",
      " 720/1053 [===================>..........] - ETA: 0sKerasRegressor\n",
      "r2 train =  0.529426843022\n",
      "r2 eval =  0.615339088082\n",
      "====================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_pd, eval_pd, models = eval_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.594231698337\n",
      "0.610931753597\n"
     ]
    }
   ],
   "source": [
    "print(r2_score(train_y, train_pd.median(axis=1)))\n",
    "print(r2_score(eval_y, eval_pd.median(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.595498721952\n",
      "0.612230206117\n"
     ]
    }
   ],
   "source": [
    "print(r2_score(train_y, train_pd[['lr', 'sgd', 'xgb', 'gbr', 'dl', 'thelisen', 'ransac']].median(axis=1)))\n",
    "print(r2_score(eval_y, eval_pd[['lr', 'sgd', 'xgb', 'gbr', 'dl', 'thelisen', 'ransac']].median(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False),\n",
       " SGDRegressor(alpha=0.0001, average=False, epsilon=0.1, eta0=0.01,\n",
       "        fit_intercept=True, l1_ratio=0.15, learning_rate='invscaling',\n",
       "        loss='squared_loss', n_iter=5, penalty='l2', power_t=0.25,\n",
       "        random_state=None, shuffle=True, verbose=0, warm_start=False),\n",
       " XGBRegressor(base_score=100.66931812782121, booster='gbtree',\n",
       "        colsample_bylevel=1, colsample_bytree=0.7, eta=0.005, gamma=0,\n",
       "        learning_rate=0.1, max_delta_step=0, max_depth=4,\n",
       "        min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
       "        nthread=1, num_parallel_tree=1, objective='reg:linear',\n",
       "        random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "        seed=0, silent=True, subsample=0.9),\n",
       " GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
       "              max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False),\n",
       " <keras.wrappers.scikit_learn.KerasRegressor at 0x1f97246d550>,\n",
       " TheilSenRegressor(copy_X=True, fit_intercept=True, max_iter=300,\n",
       "          max_subpopulation=10000, n_jobs=1, n_subsamples=None,\n",
       "          random_state=42, tol=0.001, verbose=False),\n",
       " RANSACRegressor(base_estimator=None, is_data_valid=None, is_model_valid=None,\n",
       "         loss='absolute_loss', max_trials=100, min_samples=None,\n",
       "         random_state=42, residual_metric=None, residual_threshold=None,\n",
       "         stop_n_inliers=inf, stop_probability=0.99, stop_score=inf),\n",
       " HuberRegressor(alpha=0.0001, epsilon=1.35, fit_intercept=True, max_iter=100,\n",
       "         tol=1e-05, warm_start=False)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4020/4209 [===========================>..] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "for model in models:\n",
    "    preds.append(model.predict(test_feats_encode))\n",
    "    \n",
    "dt = pd.DataFrame(data=np.array(preds).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101.110509</td>\n",
       "      <td>-2.913562e+13</td>\n",
       "      <td>80.417480</td>\n",
       "      <td>86.216553</td>\n",
       "      <td>78.688080</td>\n",
       "      <td>67.407990</td>\n",
       "      <td>76.902178</td>\n",
       "      <td>71.637042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>119.184688</td>\n",
       "      <td>1.016797e+13</td>\n",
       "      <td>102.088165</td>\n",
       "      <td>110.346816</td>\n",
       "      <td>95.085678</td>\n",
       "      <td>89.557094</td>\n",
       "      <td>88.307022</td>\n",
       "      <td>78.536183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>108.074454</td>\n",
       "      <td>-8.155418e+11</td>\n",
       "      <td>84.048294</td>\n",
       "      <td>90.212101</td>\n",
       "      <td>78.597977</td>\n",
       "      <td>77.206791</td>\n",
       "      <td>101.908711</td>\n",
       "      <td>101.377916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>78.026380</td>\n",
       "      <td>-2.111516e+13</td>\n",
       "      <td>80.986435</td>\n",
       "      <td>78.593664</td>\n",
       "      <td>78.025993</td>\n",
       "      <td>75.317603</td>\n",
       "      <td>75.067591</td>\n",
       "      <td>95.888850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>111.859218</td>\n",
       "      <td>3.206980e+12</td>\n",
       "      <td>117.740540</td>\n",
       "      <td>121.240734</td>\n",
       "      <td>113.483879</td>\n",
       "      <td>112.848198</td>\n",
       "      <td>126.080122</td>\n",
       "      <td>113.694538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>92.913721</td>\n",
       "      <td>1.970855e+13</td>\n",
       "      <td>96.415665</td>\n",
       "      <td>100.795696</td>\n",
       "      <td>94.761711</td>\n",
       "      <td>92.781880</td>\n",
       "      <td>92.666624</td>\n",
       "      <td>77.566984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>113.718284</td>\n",
       "      <td>2.271212e+13</td>\n",
       "      <td>118.565140</td>\n",
       "      <td>122.797071</td>\n",
       "      <td>113.475311</td>\n",
       "      <td>113.091965</td>\n",
       "      <td>123.369054</td>\n",
       "      <td>81.829217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>93.720759</td>\n",
       "      <td>2.689424e+13</td>\n",
       "      <td>104.095451</td>\n",
       "      <td>97.939002</td>\n",
       "      <td>95.084366</td>\n",
       "      <td>93.463658</td>\n",
       "      <td>94.215215</td>\n",
       "      <td>84.544494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>117.996842</td>\n",
       "      <td>-1.891933e+12</td>\n",
       "      <td>124.621552</td>\n",
       "      <td>125.131988</td>\n",
       "      <td>116.290230</td>\n",
       "      <td>117.978591</td>\n",
       "      <td>117.015086</td>\n",
       "      <td>103.749296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>92.384885</td>\n",
       "      <td>2.426079e+13</td>\n",
       "      <td>97.687637</td>\n",
       "      <td>93.036846</td>\n",
       "      <td>95.084427</td>\n",
       "      <td>95.513356</td>\n",
       "      <td>93.636070</td>\n",
       "      <td>100.692015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>117.797490</td>\n",
       "      <td>7.466978e+12</td>\n",
       "      <td>124.296631</td>\n",
       "      <td>124.825146</td>\n",
       "      <td>116.290283</td>\n",
       "      <td>117.852009</td>\n",
       "      <td>117.499930</td>\n",
       "      <td>89.388437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>109.335624</td>\n",
       "      <td>1.992468e+13</td>\n",
       "      <td>109.312111</td>\n",
       "      <td>111.613237</td>\n",
       "      <td>107.879051</td>\n",
       "      <td>109.802307</td>\n",
       "      <td>108.911450</td>\n",
       "      <td>99.622521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>103.644313</td>\n",
       "      <td>1.100379e+13</td>\n",
       "      <td>105.286827</td>\n",
       "      <td>113.447573</td>\n",
       "      <td>95.042877</td>\n",
       "      <td>96.777887</td>\n",
       "      <td>94.948794</td>\n",
       "      <td>100.958434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>96.420668</td>\n",
       "      <td>4.577343e+13</td>\n",
       "      <td>101.562164</td>\n",
       "      <td>98.899780</td>\n",
       "      <td>95.084190</td>\n",
       "      <td>96.668787</td>\n",
       "      <td>97.723732</td>\n",
       "      <td>102.211956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>108.625995</td>\n",
       "      <td>4.326046e+13</td>\n",
       "      <td>108.967400</td>\n",
       "      <td>113.026182</td>\n",
       "      <td>108.325081</td>\n",
       "      <td>108.781781</td>\n",
       "      <td>106.007317</td>\n",
       "      <td>95.736615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>101.657314</td>\n",
       "      <td>1.804957e+13</td>\n",
       "      <td>106.924232</td>\n",
       "      <td>113.447573</td>\n",
       "      <td>94.988380</td>\n",
       "      <td>94.251674</td>\n",
       "      <td>90.935460</td>\n",
       "      <td>100.548580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>120.392304</td>\n",
       "      <td>2.589726e+13</td>\n",
       "      <td>125.194771</td>\n",
       "      <td>124.954199</td>\n",
       "      <td>116.289757</td>\n",
       "      <td>120.514745</td>\n",
       "      <td>116.522903</td>\n",
       "      <td>92.167008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>103.658038</td>\n",
       "      <td>2.496764e+13</td>\n",
       "      <td>106.528427</td>\n",
       "      <td>113.447573</td>\n",
       "      <td>95.041557</td>\n",
       "      <td>96.793158</td>\n",
       "      <td>94.944320</td>\n",
       "      <td>101.228257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>93.607427</td>\n",
       "      <td>7.164483e+13</td>\n",
       "      <td>98.869484</td>\n",
       "      <td>96.973717</td>\n",
       "      <td>95.084564</td>\n",
       "      <td>96.028445</td>\n",
       "      <td>106.792230</td>\n",
       "      <td>103.049675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>95.128783</td>\n",
       "      <td>7.255569e+13</td>\n",
       "      <td>101.176315</td>\n",
       "      <td>97.673489</td>\n",
       "      <td>95.067482</td>\n",
       "      <td>96.891623</td>\n",
       "      <td>87.927408</td>\n",
       "      <td>92.288204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>103.655706</td>\n",
       "      <td>3.832408e+13</td>\n",
       "      <td>104.923882</td>\n",
       "      <td>111.103930</td>\n",
       "      <td>95.042046</td>\n",
       "      <td>96.790801</td>\n",
       "      <td>94.942163</td>\n",
       "      <td>101.233989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>103.654151</td>\n",
       "      <td>4.722837e+13</td>\n",
       "      <td>100.223396</td>\n",
       "      <td>101.797043</td>\n",
       "      <td>95.042366</td>\n",
       "      <td>96.789230</td>\n",
       "      <td>94.940725</td>\n",
       "      <td>101.237810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>96.047148</td>\n",
       "      <td>8.419095e+13</td>\n",
       "      <td>96.354309</td>\n",
       "      <td>93.156200</td>\n",
       "      <td>95.084625</td>\n",
       "      <td>95.230666</td>\n",
       "      <td>94.314612</td>\n",
       "      <td>110.161902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>103.586814</td>\n",
       "      <td>6.260733e+13</td>\n",
       "      <td>100.576538</td>\n",
       "      <td>102.938053</td>\n",
       "      <td>95.049568</td>\n",
       "      <td>96.715578</td>\n",
       "      <td>94.947112</td>\n",
       "      <td>100.189089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>94.396416</td>\n",
       "      <td>1.057853e+14</td>\n",
       "      <td>95.248474</td>\n",
       "      <td>94.619210</td>\n",
       "      <td>95.064079</td>\n",
       "      <td>94.439190</td>\n",
       "      <td>88.811367</td>\n",
       "      <td>80.202281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>121.117065</td>\n",
       "      <td>6.173685e+13</td>\n",
       "      <td>119.204430</td>\n",
       "      <td>119.689103</td>\n",
       "      <td>116.290276</td>\n",
       "      <td>120.696756</td>\n",
       "      <td>116.819999</td>\n",
       "      <td>99.974496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>107.534519</td>\n",
       "      <td>8.183928e+13</td>\n",
       "      <td>104.832779</td>\n",
       "      <td>106.630915</td>\n",
       "      <td>106.094597</td>\n",
       "      <td>107.877133</td>\n",
       "      <td>110.670010</td>\n",
       "      <td>88.310574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>107.472757</td>\n",
       "      <td>9.400358e+13</td>\n",
       "      <td>110.913834</td>\n",
       "      <td>107.789910</td>\n",
       "      <td>105.366158</td>\n",
       "      <td>107.870701</td>\n",
       "      <td>110.131899</td>\n",
       "      <td>92.953341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>94.995673</td>\n",
       "      <td>1.038930e+14</td>\n",
       "      <td>94.905090</td>\n",
       "      <td>94.407142</td>\n",
       "      <td>95.084404</td>\n",
       "      <td>95.120787</td>\n",
       "      <td>95.680351</td>\n",
       "      <td>108.485640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>92.971668</td>\n",
       "      <td>1.078847e+14</td>\n",
       "      <td>83.633202</td>\n",
       "      <td>82.224248</td>\n",
       "      <td>78.777946</td>\n",
       "      <td>88.813679</td>\n",
       "      <td>110.234175</td>\n",
       "      <td>94.426471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4179</th>\n",
       "      <td>113.291675</td>\n",
       "      <td>1.860560e+16</td>\n",
       "      <td>108.268623</td>\n",
       "      <td>109.509442</td>\n",
       "      <td>112.090019</td>\n",
       "      <td>112.766435</td>\n",
       "      <td>106.703869</td>\n",
       "      <td>108.273270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4180</th>\n",
       "      <td>101.019449</td>\n",
       "      <td>1.860644e+16</td>\n",
       "      <td>100.335518</td>\n",
       "      <td>101.185111</td>\n",
       "      <td>104.835625</td>\n",
       "      <td>99.228546</td>\n",
       "      <td>98.915475</td>\n",
       "      <td>110.745126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4181</th>\n",
       "      <td>94.682292</td>\n",
       "      <td>1.862168e+16</td>\n",
       "      <td>92.545357</td>\n",
       "      <td>92.872987</td>\n",
       "      <td>93.804665</td>\n",
       "      <td>94.344345</td>\n",
       "      <td>89.671635</td>\n",
       "      <td>110.946582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4182</th>\n",
       "      <td>94.996635</td>\n",
       "      <td>1.864878e+16</td>\n",
       "      <td>92.650482</td>\n",
       "      <td>92.989425</td>\n",
       "      <td>93.776283</td>\n",
       "      <td>94.329198</td>\n",
       "      <td>92.230026</td>\n",
       "      <td>88.451074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4183</th>\n",
       "      <td>105.992902</td>\n",
       "      <td>1.862305e+16</td>\n",
       "      <td>100.442001</td>\n",
       "      <td>101.389310</td>\n",
       "      <td>105.953316</td>\n",
       "      <td>106.478136</td>\n",
       "      <td>101.393155</td>\n",
       "      <td>116.509110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4184</th>\n",
       "      <td>109.286792</td>\n",
       "      <td>1.864499e+16</td>\n",
       "      <td>112.779076</td>\n",
       "      <td>110.599382</td>\n",
       "      <td>111.131371</td>\n",
       "      <td>108.979886</td>\n",
       "      <td>106.933505</td>\n",
       "      <td>114.556318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4185</th>\n",
       "      <td>93.816525</td>\n",
       "      <td>1.863002e+16</td>\n",
       "      <td>93.681580</td>\n",
       "      <td>93.768981</td>\n",
       "      <td>93.360222</td>\n",
       "      <td>93.884726</td>\n",
       "      <td>95.143071</td>\n",
       "      <td>101.092356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4186</th>\n",
       "      <td>95.431035</td>\n",
       "      <td>1.865594e+16</td>\n",
       "      <td>92.882324</td>\n",
       "      <td>93.277056</td>\n",
       "      <td>93.708435</td>\n",
       "      <td>95.849337</td>\n",
       "      <td>93.793887</td>\n",
       "      <td>110.478414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4187</th>\n",
       "      <td>130.103855</td>\n",
       "      <td>1.865294e+16</td>\n",
       "      <td>110.569633</td>\n",
       "      <td>109.442764</td>\n",
       "      <td>112.087761</td>\n",
       "      <td>111.967475</td>\n",
       "      <td>103.796799</td>\n",
       "      <td>128.042081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4188</th>\n",
       "      <td>112.068784</td>\n",
       "      <td>1.866775e+16</td>\n",
       "      <td>109.271080</td>\n",
       "      <td>109.445936</td>\n",
       "      <td>110.330032</td>\n",
       "      <td>111.976424</td>\n",
       "      <td>111.729623</td>\n",
       "      <td>119.706541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4189</th>\n",
       "      <td>89.322038</td>\n",
       "      <td>1.866138e+16</td>\n",
       "      <td>91.305679</td>\n",
       "      <td>92.670297</td>\n",
       "      <td>92.686440</td>\n",
       "      <td>89.844237</td>\n",
       "      <td>83.913924</td>\n",
       "      <td>89.179521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4190</th>\n",
       "      <td>107.306956</td>\n",
       "      <td>1.864243e+16</td>\n",
       "      <td>110.239510</td>\n",
       "      <td>110.473580</td>\n",
       "      <td>110.965958</td>\n",
       "      <td>109.824275</td>\n",
       "      <td>103.891389</td>\n",
       "      <td>95.604815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4191</th>\n",
       "      <td>89.776475</td>\n",
       "      <td>1.867947e+16</td>\n",
       "      <td>91.355431</td>\n",
       "      <td>91.557984</td>\n",
       "      <td>92.685783</td>\n",
       "      <td>89.279723</td>\n",
       "      <td>88.704347</td>\n",
       "      <td>94.408464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4192</th>\n",
       "      <td>105.313447</td>\n",
       "      <td>1.866940e+16</td>\n",
       "      <td>102.414139</td>\n",
       "      <td>100.469958</td>\n",
       "      <td>105.113190</td>\n",
       "      <td>104.491833</td>\n",
       "      <td>100.384526</td>\n",
       "      <td>110.314394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4193</th>\n",
       "      <td>100.415352</td>\n",
       "      <td>1.867870e+16</td>\n",
       "      <td>91.441948</td>\n",
       "      <td>93.036535</td>\n",
       "      <td>93.855667</td>\n",
       "      <td>101.849404</td>\n",
       "      <td>113.524745</td>\n",
       "      <td>112.647227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4194</th>\n",
       "      <td>108.193935</td>\n",
       "      <td>1.868978e+16</td>\n",
       "      <td>111.068726</td>\n",
       "      <td>109.991611</td>\n",
       "      <td>111.887672</td>\n",
       "      <td>111.034971</td>\n",
       "      <td>89.385165</td>\n",
       "      <td>109.099688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4195</th>\n",
       "      <td>93.491420</td>\n",
       "      <td>1.870617e+16</td>\n",
       "      <td>93.502502</td>\n",
       "      <td>92.888219</td>\n",
       "      <td>93.369576</td>\n",
       "      <td>94.305531</td>\n",
       "      <td>91.884127</td>\n",
       "      <td>106.136652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4196</th>\n",
       "      <td>101.454310</td>\n",
       "      <td>1.867914e+16</td>\n",
       "      <td>105.384155</td>\n",
       "      <td>101.978477</td>\n",
       "      <td>104.891319</td>\n",
       "      <td>101.529265</td>\n",
       "      <td>99.584363</td>\n",
       "      <td>122.421029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4197</th>\n",
       "      <td>104.652609</td>\n",
       "      <td>1.868026e+16</td>\n",
       "      <td>103.216133</td>\n",
       "      <td>100.963629</td>\n",
       "      <td>103.850342</td>\n",
       "      <td>105.262054</td>\n",
       "      <td>106.772302</td>\n",
       "      <td>112.769774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4198</th>\n",
       "      <td>110.592896</td>\n",
       "      <td>1.869184e+16</td>\n",
       "      <td>113.111115</td>\n",
       "      <td>109.948032</td>\n",
       "      <td>112.202072</td>\n",
       "      <td>108.246518</td>\n",
       "      <td>123.926381</td>\n",
       "      <td>109.556931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4199</th>\n",
       "      <td>90.380776</td>\n",
       "      <td>1.868686e+16</td>\n",
       "      <td>92.714714</td>\n",
       "      <td>92.239308</td>\n",
       "      <td>92.688652</td>\n",
       "      <td>89.217577</td>\n",
       "      <td>82.020452</td>\n",
       "      <td>98.162346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4200</th>\n",
       "      <td>90.865975</td>\n",
       "      <td>1.868391e+16</td>\n",
       "      <td>89.653328</td>\n",
       "      <td>91.495557</td>\n",
       "      <td>92.685448</td>\n",
       "      <td>89.912782</td>\n",
       "      <td>88.116621</td>\n",
       "      <td>112.325466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4201</th>\n",
       "      <td>93.403423</td>\n",
       "      <td>1.872978e+16</td>\n",
       "      <td>93.054489</td>\n",
       "      <td>92.519322</td>\n",
       "      <td>93.789276</td>\n",
       "      <td>92.938092</td>\n",
       "      <td>92.190121</td>\n",
       "      <td>111.051764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4202</th>\n",
       "      <td>109.761429</td>\n",
       "      <td>1.871898e+16</td>\n",
       "      <td>111.981354</td>\n",
       "      <td>108.811033</td>\n",
       "      <td>111.946014</td>\n",
       "      <td>109.477151</td>\n",
       "      <td>107.529111</td>\n",
       "      <td>119.637632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4203</th>\n",
       "      <td>107.594954</td>\n",
       "      <td>1.871315e+16</td>\n",
       "      <td>111.444641</td>\n",
       "      <td>108.640992</td>\n",
       "      <td>111.906654</td>\n",
       "      <td>107.527104</td>\n",
       "      <td>103.521819</td>\n",
       "      <td>117.238661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4204</th>\n",
       "      <td>101.411175</td>\n",
       "      <td>1.871187e+16</td>\n",
       "      <td>105.806602</td>\n",
       "      <td>101.978477</td>\n",
       "      <td>104.801704</td>\n",
       "      <td>102.493782</td>\n",
       "      <td>99.209189</td>\n",
       "      <td>121.214994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4205</th>\n",
       "      <td>97.340549</td>\n",
       "      <td>1.873505e+16</td>\n",
       "      <td>100.430527</td>\n",
       "      <td>92.330100</td>\n",
       "      <td>93.439003</td>\n",
       "      <td>95.026579</td>\n",
       "      <td>78.827847</td>\n",
       "      <td>104.844513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4206</th>\n",
       "      <td>93.528376</td>\n",
       "      <td>1.873480e+16</td>\n",
       "      <td>92.776001</td>\n",
       "      <td>92.249936</td>\n",
       "      <td>92.686516</td>\n",
       "      <td>92.001091</td>\n",
       "      <td>88.722646</td>\n",
       "      <td>111.214050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4207</th>\n",
       "      <td>108.549365</td>\n",
       "      <td>1.871204e+16</td>\n",
       "      <td>110.795906</td>\n",
       "      <td>108.994930</td>\n",
       "      <td>111.794456</td>\n",
       "      <td>108.753115</td>\n",
       "      <td>109.297651</td>\n",
       "      <td>103.892673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4208</th>\n",
       "      <td>94.029634</td>\n",
       "      <td>1.875048e+16</td>\n",
       "      <td>98.221245</td>\n",
       "      <td>92.371859</td>\n",
       "      <td>93.322914</td>\n",
       "      <td>93.136944</td>\n",
       "      <td>90.203952</td>\n",
       "      <td>92.060111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4209 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0             1           2           3           4  \\\n",
       "0     101.110509 -2.913562e+13   80.417480   86.216553   78.688080   \n",
       "1     119.184688  1.016797e+13  102.088165  110.346816   95.085678   \n",
       "2     108.074454 -8.155418e+11   84.048294   90.212101   78.597977   \n",
       "3      78.026380 -2.111516e+13   80.986435   78.593664   78.025993   \n",
       "4     111.859218  3.206980e+12  117.740540  121.240734  113.483879   \n",
       "5      92.913721  1.970855e+13   96.415665  100.795696   94.761711   \n",
       "6     113.718284  2.271212e+13  118.565140  122.797071  113.475311   \n",
       "7      93.720759  2.689424e+13  104.095451   97.939002   95.084366   \n",
       "8     117.996842 -1.891933e+12  124.621552  125.131988  116.290230   \n",
       "9      92.384885  2.426079e+13   97.687637   93.036846   95.084427   \n",
       "10    117.797490  7.466978e+12  124.296631  124.825146  116.290283   \n",
       "11    109.335624  1.992468e+13  109.312111  111.613237  107.879051   \n",
       "12    103.644313  1.100379e+13  105.286827  113.447573   95.042877   \n",
       "13     96.420668  4.577343e+13  101.562164   98.899780   95.084190   \n",
       "14    108.625995  4.326046e+13  108.967400  113.026182  108.325081   \n",
       "15    101.657314  1.804957e+13  106.924232  113.447573   94.988380   \n",
       "16    120.392304  2.589726e+13  125.194771  124.954199  116.289757   \n",
       "17    103.658038  2.496764e+13  106.528427  113.447573   95.041557   \n",
       "18     93.607427  7.164483e+13   98.869484   96.973717   95.084564   \n",
       "19     95.128783  7.255569e+13  101.176315   97.673489   95.067482   \n",
       "20    103.655706  3.832408e+13  104.923882  111.103930   95.042046   \n",
       "21    103.654151  4.722837e+13  100.223396  101.797043   95.042366   \n",
       "22     96.047148  8.419095e+13   96.354309   93.156200   95.084625   \n",
       "23    103.586814  6.260733e+13  100.576538  102.938053   95.049568   \n",
       "24     94.396416  1.057853e+14   95.248474   94.619210   95.064079   \n",
       "25    121.117065  6.173685e+13  119.204430  119.689103  116.290276   \n",
       "26    107.534519  8.183928e+13  104.832779  106.630915  106.094597   \n",
       "27    107.472757  9.400358e+13  110.913834  107.789910  105.366158   \n",
       "28     94.995673  1.038930e+14   94.905090   94.407142   95.084404   \n",
       "29     92.971668  1.078847e+14   83.633202   82.224248   78.777946   \n",
       "...          ...           ...         ...         ...         ...   \n",
       "4179  113.291675  1.860560e+16  108.268623  109.509442  112.090019   \n",
       "4180  101.019449  1.860644e+16  100.335518  101.185111  104.835625   \n",
       "4181   94.682292  1.862168e+16   92.545357   92.872987   93.804665   \n",
       "4182   94.996635  1.864878e+16   92.650482   92.989425   93.776283   \n",
       "4183  105.992902  1.862305e+16  100.442001  101.389310  105.953316   \n",
       "4184  109.286792  1.864499e+16  112.779076  110.599382  111.131371   \n",
       "4185   93.816525  1.863002e+16   93.681580   93.768981   93.360222   \n",
       "4186   95.431035  1.865594e+16   92.882324   93.277056   93.708435   \n",
       "4187  130.103855  1.865294e+16  110.569633  109.442764  112.087761   \n",
       "4188  112.068784  1.866775e+16  109.271080  109.445936  110.330032   \n",
       "4189   89.322038  1.866138e+16   91.305679   92.670297   92.686440   \n",
       "4190  107.306956  1.864243e+16  110.239510  110.473580  110.965958   \n",
       "4191   89.776475  1.867947e+16   91.355431   91.557984   92.685783   \n",
       "4192  105.313447  1.866940e+16  102.414139  100.469958  105.113190   \n",
       "4193  100.415352  1.867870e+16   91.441948   93.036535   93.855667   \n",
       "4194  108.193935  1.868978e+16  111.068726  109.991611  111.887672   \n",
       "4195   93.491420  1.870617e+16   93.502502   92.888219   93.369576   \n",
       "4196  101.454310  1.867914e+16  105.384155  101.978477  104.891319   \n",
       "4197  104.652609  1.868026e+16  103.216133  100.963629  103.850342   \n",
       "4198  110.592896  1.869184e+16  113.111115  109.948032  112.202072   \n",
       "4199   90.380776  1.868686e+16   92.714714   92.239308   92.688652   \n",
       "4200   90.865975  1.868391e+16   89.653328   91.495557   92.685448   \n",
       "4201   93.403423  1.872978e+16   93.054489   92.519322   93.789276   \n",
       "4202  109.761429  1.871898e+16  111.981354  108.811033  111.946014   \n",
       "4203  107.594954  1.871315e+16  111.444641  108.640992  111.906654   \n",
       "4204  101.411175  1.871187e+16  105.806602  101.978477  104.801704   \n",
       "4205   97.340549  1.873505e+16  100.430527   92.330100   93.439003   \n",
       "4206   93.528376  1.873480e+16   92.776001   92.249936   92.686516   \n",
       "4207  108.549365  1.871204e+16  110.795906  108.994930  111.794456   \n",
       "4208   94.029634  1.875048e+16   98.221245   92.371859   93.322914   \n",
       "\n",
       "               5           6           7  \n",
       "0      67.407990   76.902178   71.637042  \n",
       "1      89.557094   88.307022   78.536183  \n",
       "2      77.206791  101.908711  101.377916  \n",
       "3      75.317603   75.067591   95.888850  \n",
       "4     112.848198  126.080122  113.694538  \n",
       "5      92.781880   92.666624   77.566984  \n",
       "6     113.091965  123.369054   81.829217  \n",
       "7      93.463658   94.215215   84.544494  \n",
       "8     117.978591  117.015086  103.749296  \n",
       "9      95.513356   93.636070  100.692015  \n",
       "10    117.852009  117.499930   89.388437  \n",
       "11    109.802307  108.911450   99.622521  \n",
       "12     96.777887   94.948794  100.958434  \n",
       "13     96.668787   97.723732  102.211956  \n",
       "14    108.781781  106.007317   95.736615  \n",
       "15     94.251674   90.935460  100.548580  \n",
       "16    120.514745  116.522903   92.167008  \n",
       "17     96.793158   94.944320  101.228257  \n",
       "18     96.028445  106.792230  103.049675  \n",
       "19     96.891623   87.927408   92.288204  \n",
       "20     96.790801   94.942163  101.233989  \n",
       "21     96.789230   94.940725  101.237810  \n",
       "22     95.230666   94.314612  110.161902  \n",
       "23     96.715578   94.947112  100.189089  \n",
       "24     94.439190   88.811367   80.202281  \n",
       "25    120.696756  116.819999   99.974496  \n",
       "26    107.877133  110.670010   88.310574  \n",
       "27    107.870701  110.131899   92.953341  \n",
       "28     95.120787   95.680351  108.485640  \n",
       "29     88.813679  110.234175   94.426471  \n",
       "...          ...         ...         ...  \n",
       "4179  112.766435  106.703869  108.273270  \n",
       "4180   99.228546   98.915475  110.745126  \n",
       "4181   94.344345   89.671635  110.946582  \n",
       "4182   94.329198   92.230026   88.451074  \n",
       "4183  106.478136  101.393155  116.509110  \n",
       "4184  108.979886  106.933505  114.556318  \n",
       "4185   93.884726   95.143071  101.092356  \n",
       "4186   95.849337   93.793887  110.478414  \n",
       "4187  111.967475  103.796799  128.042081  \n",
       "4188  111.976424  111.729623  119.706541  \n",
       "4189   89.844237   83.913924   89.179521  \n",
       "4190  109.824275  103.891389   95.604815  \n",
       "4191   89.279723   88.704347   94.408464  \n",
       "4192  104.491833  100.384526  110.314394  \n",
       "4193  101.849404  113.524745  112.647227  \n",
       "4194  111.034971   89.385165  109.099688  \n",
       "4195   94.305531   91.884127  106.136652  \n",
       "4196  101.529265   99.584363  122.421029  \n",
       "4197  105.262054  106.772302  112.769774  \n",
       "4198  108.246518  123.926381  109.556931  \n",
       "4199   89.217577   82.020452   98.162346  \n",
       "4200   89.912782   88.116621  112.325466  \n",
       "4201   92.938092   92.190121  111.051764  \n",
       "4202  109.477151  107.529111  119.637632  \n",
       "4203  107.527104  103.521819  117.238661  \n",
       "4204  102.493782   99.209189  121.214994  \n",
       "4205   95.026579   78.827847  104.844513  \n",
       "4206   92.001091   88.722646  111.214050  \n",
       "4207  108.753115  109.297651  103.892673  \n",
       "4208   93.136944   90.203952   92.060111  \n",
       "\n",
       "[4209 rows x 8 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
