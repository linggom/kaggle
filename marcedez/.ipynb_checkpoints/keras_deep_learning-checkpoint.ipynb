{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Righnow candidate is : SVR with PCA n_components = 10 with results 0.23706564506631833"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X8</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X375</th>\n",
       "      <th>X376</th>\n",
       "      <th>X377</th>\n",
       "      <th>X378</th>\n",
       "      <th>X379</th>\n",
       "      <th>X380</th>\n",
       "      <th>X382</th>\n",
       "      <th>X383</th>\n",
       "      <th>X384</th>\n",
       "      <th>X385</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>az</td>\n",
       "      <td>v</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>d</td>\n",
       "      <td>t</td>\n",
       "      <td>a</td>\n",
       "      <td>w</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>t</td>\n",
       "      <td>b</td>\n",
       "      <td>ai</td>\n",
       "      <td>a</td>\n",
       "      <td>d</td>\n",
       "      <td>b</td>\n",
       "      <td>g</td>\n",
       "      <td>y</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>az</td>\n",
       "      <td>v</td>\n",
       "      <td>as</td>\n",
       "      <td>f</td>\n",
       "      <td>d</td>\n",
       "      <td>a</td>\n",
       "      <td>j</td>\n",
       "      <td>j</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>az</td>\n",
       "      <td>l</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>d</td>\n",
       "      <td>z</td>\n",
       "      <td>l</td>\n",
       "      <td>n</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>w</td>\n",
       "      <td>s</td>\n",
       "      <td>as</td>\n",
       "      <td>c</td>\n",
       "      <td>d</td>\n",
       "      <td>y</td>\n",
       "      <td>i</td>\n",
       "      <td>m</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 377 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  X0 X1  X2 X3 X4 X5 X6 X8  X10  ...   X375  X376  X377  X378  X379  \\\n",
       "0   1  az  v   n  f  d  t  a  w    0  ...      0     0     0     1     0   \n",
       "1   2   t  b  ai  a  d  b  g  y    0  ...      0     0     1     0     0   \n",
       "2   3  az  v  as  f  d  a  j  j    0  ...      0     0     0     1     0   \n",
       "3   4  az  l   n  f  d  z  l  n    0  ...      0     0     0     1     0   \n",
       "4   5   w  s  as  c  d  y  i  m    0  ...      1     0     0     0     0   \n",
       "\n",
       "   X380  X382  X383  X384  X385  \n",
       "0     0     0     0     0     0  \n",
       "1     0     0     0     0     0  \n",
       "2     0     0     0     0     0  \n",
       "3     0     0     0     0     0  \n",
       "4     0     0     0     0     0  \n",
       "\n",
       "[5 rows x 377 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels = train.iloc[:,1]\n",
    "train_feats = train.iloc[:, 2:]\n",
    "\n",
    "test_labels = test.iloc[:,1]\n",
    "test_feats = test.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_feats_encode = []\n",
    "train_feats_encode.append(train.iloc[:, 0])\n",
    "for i in range(train_feats.shape[1]):\n",
    "    arr = train_feats.iloc[:, i]\n",
    "    if arr.dtype == 'O':        \n",
    "        lblencod = LabelEncoder()\n",
    "        arr = lblencod.fit_transform(arr)\n",
    "    train_feats_encode.append(arr)\n",
    "train_feats_encode = np.array(train_feats_encode).T\n",
    "\n",
    "test_feats_encode = []\n",
    "test_feats_encode.append(test.iloc[:, 0])\n",
    "for i in range(test_feats.shape[1]):\n",
    "    arr = test_feats.iloc[:, i]\n",
    "    if arr.dtype == 'O':        \n",
    "        lblencod = LabelEncoder()\n",
    "        arr = lblencod.fit_transform(arr)\n",
    "    test_feats_encode.append(arr)\n",
    "test_feats_encode = np.array(test_feats_encode).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Keras Deep Learning ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# define custom R2 metrics for Keras backend\n",
    "from keras import backend as K\n",
    "# to tune the NN\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD, Adam\n",
    "act_func = 'tanh'\n",
    "\n",
    "def r2_keras(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square( y_true - y_pred )) \n",
    "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) ) \n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model():\n",
    "    model = Sequential()\n",
    "    # Input layer with dimension input_dims and hidden layer i with input_dims neurons. \n",
    "    model.add(Dense(input_dims, input_dim=input_dims, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Activation(\"linear\"))\n",
    "    # Hidden layer\n",
    "    model.add(Dense(input_dims, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Activation(\"linear\"))\n",
    "    # Hidden layer\n",
    "    model.add(Dense(input_dims, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Activation(\"linear\"))\n",
    "    # Hidden layer\n",
    "    model.add(Dense(input_dims, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Activation(\"linear\"))\n",
    "    # Hidden layer\n",
    "    model.add(Dense(input_dims//2, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Activation(\"linear\"))\n",
    "    # Output Layer.\n",
    "    model.add(Dense(1))\n",
    "    # Use a large learning rate with decay and a large momentum. \n",
    "    # Increase your learning rate by a factor of 10 to 100 and use a high momentum value of 0.9 or 0.99\n",
    "    # sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    # adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    # compile this model\n",
    "    model.compile(loss='mean_squared_error', # one may use 'mean_absolute_error' as alternative\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=[r2_keras,\"mse\"] # you can add several if needed\n",
    "                 )\n",
    "    \n",
    "    # Visualize NN architecture\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_dl(train_X, eval_X, train_y, eval_y):\n",
    "    input_dims = train_X.shape[1]\n",
    "    estimator = KerasRegressor(\n",
    "        build_fn=model, \n",
    "        nb_epoch=300, \n",
    "        batch_size=20,\n",
    "        verbose=1\n",
    "    )\n",
    "    estimator.fit(\n",
    "        train_X, \n",
    "        train_y, \n",
    "        shuffle=True,\n",
    "        validation_data=(eval_X, eval_y),\n",
    "    )\n",
    "    prediction_train  = estimator.predict(train_X)\n",
    "    score = r2_score(train_y, prediction_train)\n",
    "    print(\"\\ntrain : \", score) \n",
    "    if eval_X is not None and eval_y is not none:\n",
    "        prediction_eval = estimator.predict(eval_X)\n",
    "        score = r2_score(eval_y, prediction_eval)\n",
    "        print(\"\\ntest : \", score)     \n",
    "    return estimator, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_41 (Dense)                 (None, 377)           142506      dense_input_9[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_25 (BatchNorma(None, 377)           754         dense_41[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_25 (Activation)       (None, 377)           0           batchnormalization_25[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)             (None, 377)           0           activation_25[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_42 (Dense)                 (None, 377)           142506      dropout_25[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_26 (BatchNorma(None, 377)           754         dense_42[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_26 (Activation)       (None, 377)           0           batchnormalization_26[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)             (None, 377)           0           activation_26[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_43 (Dense)                 (None, 188)           71064       dropout_26[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_27 (BatchNorma(None, 188)           376         dense_43[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_27 (Activation)       (None, 188)           0           batchnormalization_27[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)             (None, 188)           0           activation_27[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_44 (Dense)                 (None, 94)            17766       dropout_27[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_45 (Dense)                 (None, 1)             95          dense_44[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 375821\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Train on 3156 samples, validate on 1053 samples\n",
      "Epoch 1/300\n",
      "3156/3156 [==============================] - 13s - loss: 8851.2676 - r2_keras: -68.6785 - val_loss: 7964.3949 - val_r2_keras: -59.4792\n",
      "Epoch 2/300\n",
      "3156/3156 [==============================] - 5s - loss: 4241.7503 - r2_keras: -32.6753 - val_loss: 3465.0506 - val_r2_keras: -25.1300\n",
      "Epoch 3/300\n",
      "3156/3156 [==============================] - 5s - loss: 2159.0486 - r2_keras: -16.0535 - val_loss: 1588.6123 - val_r2_keras: -10.7360\n",
      "Epoch 4/300\n",
      "3156/3156 [==============================] - 5s - loss: 1206.0567 - r2_keras: -8.1235 - val_loss: 881.2785 - val_r2_keras: -5.3850\n",
      "Epoch 5/300\n",
      "3156/3156 [==============================] - 5s - loss: 674.8456 - r2_keras: -4.0796 - val_loss: 500.2302 - val_r2_keras: -2.5271\n",
      "Epoch 6/300\n",
      "3156/3156 [==============================] - 4s - loss: 395.7079 - r2_keras: -1.8514 - val_loss: 306.6461 - val_r2_keras: -1.0943\n",
      "Epoch 7/300\n",
      "3156/3156 [==============================] - 4s - loss: 257.9082 - r2_keras: -0.7663 - val_loss: 216.5059 - val_r2_keras: -0.4423\n",
      "Epoch 8/300\n",
      "3156/3156 [==============================] - 6s - loss: 197.5210 - r2_keras: -0.3169 - val_loss: 179.3320 - val_r2_keras: -0.1845\n",
      "Epoch 9/300\n",
      "3156/3156 [==============================] - 5s - loss: 173.7420 - r2_keras: -0.1278 - val_loss: 165.2869 - val_r2_keras: -0.0948\n",
      "Epoch 10/300\n",
      "3156/3156 [==============================] - 5s - loss: 165.1700 - r2_keras: -0.0995 - val_loss: 160.5875 - val_r2_keras: -0.0699\n",
      "Epoch 11/300\n",
      "3156/3156 [==============================] - 4s - loss: 162.6693 - r2_keras: -0.0647 - val_loss: 159.3539 - val_r2_keras: -0.0665\n",
      "Epoch 12/300\n",
      "3156/3156 [==============================] - 6s - loss: 161.3952 - r2_keras: -0.0470 - val_loss: 158.9776 - val_r2_keras: -0.0671\n",
      "Epoch 13/300\n",
      "3156/3156 [==============================] - 5s - loss: 161.2386 - r2_keras: -0.0748 - val_loss: 158.9268 - val_r2_keras: -0.0687\n",
      "Epoch 14/300\n",
      "3156/3156 [==============================] - 4s - loss: 162.1087 - r2_keras: -0.0672 - val_loss: 157.9901 - val_r2_keras: -0.0627\n",
      "Epoch 15/300\n",
      "3156/3156 [==============================] - 5s - loss: 161.4196 - r2_keras: -0.0813 - val_loss: 158.3714 - val_r2_keras: -0.0661\n",
      "Epoch 16/300\n",
      "3156/3156 [==============================] - 4s - loss: 161.3917 - r2_keras: -0.0661 - val_loss: 158.7951 - val_r2_keras: -0.0701\n",
      "Epoch 17/300\n",
      "3156/3156 [==============================] - 4s - loss: 161.3066 - r2_keras: -0.0682 - val_loss: 158.8836 - val_r2_keras: -0.0698\n",
      "Epoch 18/300\n",
      "3156/3156 [==============================] - 4s - loss: 161.5054 - r2_keras: -0.0663 - val_loss: 157.3489 - val_r2_keras: -0.0552\n",
      "Epoch 19/300\n",
      "3156/3156 [==============================] - 4s - loss: 161.3681 - r2_keras: -0.0639 - val_loss: 158.8278 - val_r2_keras: -0.0691\n",
      "Epoch 20/300\n",
      "3156/3156 [==============================] - 4s - loss: 162.8884 - r2_keras: -0.0660 - val_loss: 158.8981 - val_r2_keras: -0.0705\n",
      "Epoch 21/300\n",
      "3156/3156 [==============================] - 4s - loss: 161.4891 - r2_keras: -0.0542 - val_loss: 158.9153 - val_r2_keras: -0.0702\n",
      "Epoch 22/300\n",
      "3156/3156 [==============================] - 4s - loss: 164.3826 - r2_keras: -0.0834 - val_loss: 158.7111 - val_r2_keras: -0.0693\n",
      "Epoch 23/300\n",
      "3156/3156 [==============================] - 4s - loss: 161.4456 - r2_keras: -0.0618 - val_loss: 158.9209 - val_r2_keras: -0.0686\n",
      "Epoch 24/300\n",
      "3156/3156 [==============================] - 3s - loss: 161.3562 - r2_keras: -0.0563 - val_loss: 158.9095 - val_r2_keras: -0.0691\n",
      "Epoch 25/300\n",
      "3156/3156 [==============================] - 4s - loss: 161.4513 - r2_keras: -0.0614 - val_loss: 158.8583 - val_r2_keras: -0.0685\n",
      "Epoch 26/300\n",
      "3156/3156 [==============================] - 4s - loss: 160.9231 - r2_keras: -0.0566 - val_loss: 158.6520 - val_r2_keras: -0.0654\n",
      "Epoch 27/300\n",
      "3156/3156 [==============================] - 4s - loss: 160.2096 - r2_keras: -0.0383 - val_loss: 158.2303 - val_r2_keras: -0.0692\n",
      "Epoch 28/300\n",
      "3156/3156 [==============================] - 3s - loss: 160.0987 - r2_keras: -0.0622 - val_loss: 158.0626 - val_r2_keras: -0.0651\n",
      "Epoch 29/300\n",
      "3156/3156 [==============================] - 4s - loss: 160.2356 - r2_keras: -0.0498 - val_loss: 159.1738 - val_r2_keras: -0.0694\n",
      "Epoch 30/300\n",
      "3156/3156 [==============================] - 3s - loss: 160.4102 - r2_keras: -0.0488 - val_loss: 158.3926 - val_r2_keras: -0.0734\n",
      "Epoch 31/300\n",
      "3156/3156 [==============================] - 4s - loss: 159.6718 - r2_keras: -0.0577 - val_loss: 159.2108 - val_r2_keras: -0.0770\n",
      "Epoch 32/300\n",
      "3156/3156 [==============================] - 4s - loss: 161.1303 - r2_keras: -0.0602 - val_loss: 158.0573 - val_r2_keras: -0.0668\n",
      "Epoch 33/300\n",
      "3156/3156 [==============================] - 5s - loss: 159.9804 - r2_keras: -0.0527 - val_loss: 159.4107 - val_r2_keras: -0.0679\n",
      "Epoch 34/300\n",
      "3156/3156 [==============================] - 4s - loss: 158.3674 - r2_keras: -0.0388 - val_loss: 165.0461 - val_r2_keras: -0.0989\n",
      "Epoch 35/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3156/3156 [==============================] - 4s - loss: 156.3863 - r2_keras: -0.0334 - val_loss: 156.7013 - val_r2_keras: -0.0457\n",
      "Epoch 36/300\n",
      "3156/3156 [==============================] - 3s - loss: 150.9913 - r2_keras: 0.0086 - val_loss: 166.7905 - val_r2_keras: -0.1099\n",
      "Epoch 37/300\n",
      "3156/3156 [==============================] - 3s - loss: 147.5514 - r2_keras: 0.0243 - val_loss: 150.9727 - val_r2_keras: -0.0271\n",
      "Epoch 38/300\n",
      "3156/3156 [==============================] - 3s - loss: 138.0057 - r2_keras: 0.1213 - val_loss: 161.3844 - val_r2_keras: -0.0641\n",
      "Epoch 39/300\n",
      "3156/3156 [==============================] - 4s - loss: 131.0163 - r2_keras: 0.1502 - val_loss: 168.6712 - val_r2_keras: -0.1845\n",
      "Epoch 40/300\n",
      "3156/3156 [==============================] - 4s - loss: 125.7380 - r2_keras: 0.2068 - val_loss: 178.7100 - val_r2_keras: -0.2712\n",
      "Epoch 41/300\n",
      "3156/3156 [==============================] - 3s - loss: 113.1340 - r2_keras: 0.2771 - val_loss: 97.0453 - val_r2_keras: 0.3986\n",
      "Epoch 42/300\n",
      "3156/3156 [==============================] - 4s - loss: 108.2595 - r2_keras: 0.3147 - val_loss: 127.9812 - val_r2_keras: 0.1332\n",
      "Epoch 43/300\n",
      "3156/3156 [==============================] - 3s - loss: 106.4743 - r2_keras: 0.3318 - val_loss: 101.0112 - val_r2_keras: 0.3706\n",
      "Epoch 44/300\n",
      "3156/3156 [==============================] - 4s - loss: 105.1761 - r2_keras: 0.3472 - val_loss: 88.4073 - val_r2_keras: 0.4602\n",
      "Epoch 45/300\n",
      "3156/3156 [==============================] - 4s - loss: 100.4240 - r2_keras: 0.3567 - val_loss: 85.9280 - val_r2_keras: 0.4687\n",
      "Epoch 46/300\n",
      "3156/3156 [==============================] - 4s - loss: 98.3633 - r2_keras: 0.3848 - val_loss: 172.5152 - val_r2_keras: -0.2493\n",
      "Epoch 47/300\n",
      "3156/3156 [==============================] - 3s - loss: 98.1265 - r2_keras: 0.3885 - val_loss: 88.3632 - val_r2_keras: 0.4645\n",
      "Epoch 48/300\n",
      "3156/3156 [==============================] - 3s - loss: 97.2545 - r2_keras: 0.3827 - val_loss: 84.4033 - val_r2_keras: 0.4893\n",
      "Epoch 49/300\n",
      "3156/3156 [==============================] - 3s - loss: 96.6765 - r2_keras: 0.3836 - val_loss: 85.9619 - val_r2_keras: 0.4811\n",
      "Epoch 50/300\n",
      "3156/3156 [==============================] - 4s - loss: 95.6244 - r2_keras: 0.4092 - val_loss: 111.5662 - val_r2_keras: 0.3133\n",
      "Epoch 51/300\n",
      "3156/3156 [==============================] - 3s - loss: 94.6932 - r2_keras: 0.4117 - val_loss: 89.4018 - val_r2_keras: 0.4328\n",
      "Epoch 52/300\n",
      "3156/3156 [==============================] - 3s - loss: 91.0427 - r2_keras: 0.4466 - val_loss: 109.0594 - val_r2_keras: 0.2664\n",
      "Epoch 53/300\n",
      "3156/3156 [==============================] - 3s - loss: 91.9706 - r2_keras: 0.4346 - val_loss: 106.2930 - val_r2_keras: 0.3403\n",
      "Epoch 54/300\n",
      "3156/3156 [==============================] - 3s - loss: 91.7325 - r2_keras: 0.4244 - val_loss: 80.8253 - val_r2_keras: 0.5054\n",
      "Epoch 55/300\n",
      "3156/3156 [==============================] - 3s - loss: 92.3638 - r2_keras: 0.4325 - val_loss: 76.1294 - val_r2_keras: 0.5344\n",
      "Epoch 56/300\n",
      "3156/3156 [==============================] - 4s - loss: 89.0514 - r2_keras: 0.4520 - val_loss: 102.6830 - val_r2_keras: 0.3686\n",
      "Epoch 57/300\n",
      "3156/3156 [==============================] - 4s - loss: 86.0410 - r2_keras: 0.4694 - val_loss: 76.4171 - val_r2_keras: 0.5422\n",
      "Epoch 58/300\n",
      "3156/3156 [==============================] - 4s - loss: 84.6242 - r2_keras: 0.4890 - val_loss: 76.2891 - val_r2_keras: 0.5422\n",
      "Epoch 59/300\n",
      "3156/3156 [==============================] - 4s - loss: 83.9001 - r2_keras: 0.4841 - val_loss: 102.9045 - val_r2_keras: 0.3060\n",
      "Epoch 60/300\n",
      "3156/3156 [==============================] - 3s - loss: 85.8995 - r2_keras: 0.4761 - val_loss: 72.9658 - val_r2_keras: 0.5609\n",
      "Epoch 61/300\n",
      "3156/3156 [==============================] - 4s - loss: 88.3426 - r2_keras: 0.4483 - val_loss: 86.3341 - val_r2_keras: 0.4420\n",
      "Epoch 62/300\n",
      "3156/3156 [==============================] - 4s - loss: 85.7579 - r2_keras: 0.4697 - val_loss: 86.3993 - val_r2_keras: 0.4823\n",
      "Epoch 63/300\n",
      "3156/3156 [==============================] - 3s - loss: 84.2956 - r2_keras: 0.4883 - val_loss: 81.7600 - val_r2_keras: 0.4774\n",
      "Epoch 64/300\n",
      "3156/3156 [==============================] - 3s - loss: 82.6019 - r2_keras: 0.4995 - val_loss: 120.6486 - val_r2_keras: 0.2278\n",
      "Epoch 65/300\n",
      "3156/3156 [==============================] - 5s - loss: 85.0057 - r2_keras: 0.4740 - val_loss: 125.4496 - val_r2_keras: 0.2112\n",
      "Epoch 66/300\n",
      "3156/3156 [==============================] - 4s - loss: 80.9994 - r2_keras: 0.5100 - val_loss: 79.6837 - val_r2_keras: 0.5095\n",
      "Epoch 67/300\n",
      "3156/3156 [==============================] - 4s - loss: 80.6628 - r2_keras: 0.5185 - val_loss: 75.5472 - val_r2_keras: 0.5540\n",
      "Epoch 68/300\n",
      "3156/3156 [==============================] - 4s - loss: 81.5115 - r2_keras: 0.5164 - val_loss: 85.6059 - val_r2_keras: 0.4853\n",
      "Epoch 69/300\n",
      "3156/3156 [==============================] - 4s - loss: 81.6861 - r2_keras: 0.5072 - val_loss: 84.6630 - val_r2_keras: 0.4969\n",
      "Epoch 70/300\n",
      "3156/3156 [==============================] - 4s - loss: 80.3861 - r2_keras: 0.5138 - val_loss: 87.6461 - val_r2_keras: 0.4735\n",
      "Epoch 71/300\n",
      "3156/3156 [==============================] - 5s - loss: 83.9567 - r2_keras: 0.4876 - val_loss: 80.4794 - val_r2_keras: 0.5264\n",
      "Epoch 72/300\n",
      "3156/3156 [==============================] - 4s - loss: 79.7368 - r2_keras: 0.5229 - val_loss: 85.6854 - val_r2_keras: 0.4498\n",
      "Epoch 73/300\n",
      "3156/3156 [==============================] - 3s - loss: 81.2295 - r2_keras: 0.5077 - val_loss: 78.4073 - val_r2_keras: 0.5380\n",
      "Epoch 74/300\n",
      "3156/3156 [==============================] - 3s - loss: 82.1916 - r2_keras: 0.4846 - val_loss: 75.7783 - val_r2_keras: 0.5525\n",
      "Epoch 75/300\n",
      "3156/3156 [==============================] - 3s - loss: 82.8093 - r2_keras: 0.4547 - val_loss: 79.6262 - val_r2_keras: 0.5269\n",
      "Epoch 76/300\n",
      "3156/3156 [==============================] - 3s - loss: 79.9319 - r2_keras: 0.5173 - val_loss: 74.8141 - val_r2_keras: 0.5575\n",
      "Epoch 77/300\n",
      "3156/3156 [==============================] - 3s - loss: 80.4304 - r2_keras: 0.5175 - val_loss: 138.9439 - val_r2_keras: 0.0122\n",
      "Epoch 78/300\n",
      "3156/3156 [==============================] - 3s - loss: 78.2556 - r2_keras: 0.5302 - val_loss: 72.2527 - val_r2_keras: 0.5646\n",
      "Epoch 79/300\n",
      "3156/3156 [==============================] - 4s - loss: 77.8031 - r2_keras: 0.5418 - val_loss: 111.8192 - val_r2_keras: 0.2436\n",
      "Epoch 80/300\n",
      "3156/3156 [==============================] - 4s - loss: 79.3230 - r2_keras: 0.5204 - val_loss: 76.2937 - val_r2_keras: 0.5446\n",
      "Epoch 81/300\n",
      "3156/3156 [==============================] - 4s - loss: 78.0426 - r2_keras: 0.5403 - val_loss: 76.0238 - val_r2_keras: 0.5380\n",
      "Epoch 82/300\n",
      "3156/3156 [==============================] - 4s - loss: 77.9757 - r2_keras: 0.5244 - val_loss: 76.6475 - val_r2_keras: 0.5461\n",
      "Epoch 83/300\n",
      "3156/3156 [==============================] - 4s - loss: 79.3171 - r2_keras: 0.5254 - val_loss: 131.6368 - val_r2_keras: 0.1644\n",
      "Epoch 84/300\n",
      "3156/3156 [==============================] - 4s - loss: 78.2151 - r2_keras: 0.5274 - val_loss: 90.1546 - val_r2_keras: 0.4575\n",
      "Epoch 85/300\n",
      "3156/3156 [==============================] - 4s - loss: 81.9825 - r2_keras: 0.4832 - val_loss: 79.8705 - val_r2_keras: 0.5164\n",
      "Epoch 86/300\n",
      "3156/3156 [==============================] - 4s - loss: 79.4449 - r2_keras: 0.5219 - val_loss: 76.8115 - val_r2_keras: 0.5442\n",
      "Epoch 87/300\n",
      "3156/3156 [==============================] - 3s - loss: 79.7378 - r2_keras: 0.5360 - val_loss: 75.1450 - val_r2_keras: 0.5381\n",
      "Epoch 88/300\n",
      "3156/3156 [==============================] - 4s - loss: 78.2283 - r2_keras: 0.5276 - val_loss: 71.7827 - val_r2_keras: 0.5779\n",
      "Epoch 89/300\n",
      "3156/3156 [==============================] - 4s - loss: 79.5450 - r2_keras: 0.5141 - val_loss: 136.8030 - val_r2_keras: 0.0334\n",
      "Epoch 90/300\n",
      "3156/3156 [==============================] - 4s - loss: 81.1552 - r2_keras: 0.5118 - val_loss: 79.4902 - val_r2_keras: 0.5335\n",
      "Epoch 91/300\n",
      "3156/3156 [==============================] - 4s - loss: 79.3930 - r2_keras: 0.5158 - val_loss: 119.9345 - val_r2_keras: 0.2526\n",
      "Epoch 92/300\n",
      "3156/3156 [==============================] - 5s - loss: 79.1502 - r2_keras: 0.5263 - val_loss: 73.0135 - val_r2_keras: 0.56830.\n",
      "Epoch 93/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3156/3156 [==============================] - 5s - loss: 77.9375 - r2_keras: 0.5337 - val_loss: 130.9458 - val_r2_keras: 0.0781\n",
      "Epoch 94/300\n",
      "3156/3156 [==============================] - 5s - loss: 78.0555 - r2_keras: 0.5420 - val_loss: 75.1767 - val_r2_keras: 0.5472\n",
      "Epoch 95/300\n",
      "3156/3156 [==============================] - 4s - loss: 79.4748 - r2_keras: 0.5220 - val_loss: 74.7850 - val_r2_keras: 0.5634\n",
      "Epoch 96/300\n",
      "3156/3156 [==============================] - 5s - loss: 77.3195 - r2_keras: 0.5356 - val_loss: 81.3127 - val_r2_keras: 0.5120\n",
      "Epoch 97/300\n",
      "3156/3156 [==============================] - 5s - loss: 78.2833 - r2_keras: 0.5376 - val_loss: 92.6286 - val_r2_keras: 0.4383\n",
      "Epoch 98/300\n",
      "3156/3156 [==============================] - 5s - loss: 80.3506 - r2_keras: 0.5116 - val_loss: 74.0485 - val_r2_keras: 0.5620\n",
      "Epoch 99/300\n",
      "3156/3156 [==============================] - 5s - loss: 78.5517 - r2_keras: 0.5260 - val_loss: 79.2833 - val_r2_keras: 0.5310\n",
      "Epoch 100/300\n",
      "3156/3156 [==============================] - 4s - loss: 77.5737 - r2_keras: 0.5206 - val_loss: 79.9258 - val_r2_keras: 0.5176\n",
      "Epoch 101/300\n",
      "3156/3156 [==============================] - 3s - loss: 78.4143 - r2_keras: 0.5288 - val_loss: 73.3765 - val_r2_keras: 0.5663\n",
      "Epoch 102/300\n",
      "3156/3156 [==============================] - 3s - loss: 75.8825 - r2_keras: 0.5490 - val_loss: 72.1970 - val_r2_keras: 0.5636\n",
      "Epoch 103/300\n",
      "3156/3156 [==============================] - 3s - loss: 77.7635 - r2_keras: 0.5309 - val_loss: 78.7179 - val_r2_keras: 0.5330\n",
      "Epoch 104/300\n",
      "3156/3156 [==============================] - 3s - loss: 77.8461 - r2_keras: 0.5274 - val_loss: 82.8103 - val_r2_keras: 0.4987\n",
      "Epoch 105/300\n",
      "3156/3156 [==============================] - 3s - loss: 75.6185 - r2_keras: 0.5474 - val_loss: 72.1711 - val_r2_keras: 0.5777\n",
      "Epoch 106/300\n",
      "3156/3156 [==============================] - 3s - loss: 76.2238 - r2_keras: 0.5455 - val_loss: 77.3265 - val_r2_keras: 0.53490.562 - ETA: 1s - loss: 81.6 -\n",
      "Epoch 107/300\n",
      "3156/3156 [==============================] - 3s - loss: 76.5874 - r2_keras: 0.5446 - val_loss: 79.7206 - val_r2_keras: 0.5305- ETA: 0s - loss: 76.6858 - r2_keras: 0.543 - ETA: 0s - loss: 76.7755 - r2_keras: 0.54\n",
      "Epoch 108/300\n",
      "3156/3156 [==============================] - 3s - loss: 74.6293 - r2_keras: 0.5556 - val_loss: 77.2517 - val_r2_keras: 0.5425\n",
      "Epoch 109/300\n",
      "3156/3156 [==============================] - 3s - loss: 77.5004 - r2_keras: 0.5227 - val_loss: 70.4429 - val_r2_keras: 0.5838\n",
      "Epoch 110/300\n",
      "3156/3156 [==============================] - 3s - loss: 77.2609 - r2_keras: 0.5338 - val_loss: 71.4123 - val_r2_keras: 0.5760A: 0s - loss: 78.6644 - r2_k\n",
      "Epoch 111/300\n",
      "3156/3156 [==============================] - 3s - loss: 78.9480 - r2_keras: 0.5144 - val_loss: 114.2600 - val_r2_keras: 0.2851\n",
      "Epoch 112/300\n",
      "3156/3156 [==============================] - 3s - loss: 76.7243 - r2_keras: 0.5466 - val_loss: 82.3279 - val_r2_keras: 0.4862\n",
      "Epoch 113/300\n",
      "3156/3156 [==============================] - 3s - loss: 76.5257 - r2_keras: 0.5421 - val_loss: 72.3508 - val_r2_keras: 0.5734\n",
      "Epoch 114/300\n",
      "3156/3156 [==============================] - 4s - loss: 76.8841 - r2_keras: 0.5408 - val_loss: 125.5734 - val_r2_keras: 0.1930\n",
      "Epoch 115/300\n",
      "3156/3156 [==============================] - 3s - loss: 77.4410 - r2_keras: 0.5382 - val_loss: 80.7967 - val_r2_keras: 0.4945\n",
      "Epoch 116/300\n",
      "3156/3156 [==============================] - 3s - loss: 76.7192 - r2_keras: 0.5460 - val_loss: 72.7904 - val_r2_keras: 0.5652\n",
      "Epoch 117/300\n",
      "3156/3156 [==============================] - 3s - loss: 77.8136 - r2_keras: 0.5458 - val_loss: 80.1489 - val_r2_keras: 0.5063\n",
      "Epoch 118/300\n",
      "3156/3156 [==============================] - 3s - loss: 76.4948 - r2_keras: 0.5335 - val_loss: 71.7293 - val_r2_keras: 0.5743\n",
      "Epoch 119/300\n",
      "3156/3156 [==============================] - 4s - loss: 75.3250 - r2_keras: 0.5449 - val_loss: 76.6931 - val_r2_keras: 0.5380\n",
      "Epoch 120/300\n",
      "3156/3156 [==============================] - 4s - loss: 74.7223 - r2_keras: 0.5552 - val_loss: 72.2188 - val_r2_keras: 0.5625\n",
      "Epoch 121/300\n",
      "3156/3156 [==============================] - 4s - loss: 76.9170 - r2_keras: 0.5247 - val_loss: 70.2982 - val_r2_keras: 0.5772\n",
      "Epoch 122/300\n",
      "3156/3156 [==============================] - 4s - loss: 76.2302 - r2_keras: 0.5464 - val_loss: 73.3094 - val_r2_keras: 0.5675\n",
      "Epoch 123/300\n",
      "3156/3156 [==============================] - 4s - loss: 75.0178 - r2_keras: 0.5655 - val_loss: 70.3205 - val_r2_keras: 0.5807\n",
      "Epoch 124/300\n",
      "3156/3156 [==============================] - 4s - loss: 74.5028 - r2_keras: 0.5651 - val_loss: 78.4879 - val_r2_keras: 0.5406\n",
      "Epoch 125/300\n",
      "3156/3156 [==============================] - 4s - loss: 77.0816 - r2_keras: 0.5442 - val_loss: 83.9381 - val_r2_keras: 0.4955\n",
      "Epoch 126/300\n",
      "3156/3156 [==============================] - 4s - loss: 75.7545 - r2_keras: 0.5504 - val_loss: 76.3311 - val_r2_keras: 0.5533\n",
      "Epoch 127/300\n",
      "3156/3156 [==============================] - 4s - loss: 75.3415 - r2_keras: 0.5535 - val_loss: 74.3013 - val_r2_keras: 0.5616\n",
      "Epoch 128/300\n",
      "3156/3156 [==============================] - 4s - loss: 76.3024 - r2_keras: 0.5467 - val_loss: 89.1702 - val_r2_keras: 0.4350\n",
      "Epoch 129/300\n",
      "3156/3156 [==============================] - 4s - loss: 76.1957 - r2_keras: 0.5456 - val_loss: 97.1538 - val_r2_keras: 0.40310s -\n",
      "Epoch 130/300\n",
      "3156/3156 [==============================] - 3s - loss: 79.4405 - r2_keras: 0.5187 - val_loss: 93.3642 - val_r2_keras: 0.4333\n",
      "Epoch 131/300\n",
      "3156/3156 [==============================] - 4s - loss: 78.1219 - r2_keras: 0.5376 - val_loss: 77.2860 - val_r2_keras: 0.5417\n",
      "Epoch 132/300\n",
      "3156/3156 [==============================] - 3s - loss: 75.9415 - r2_keras: 0.5464 - val_loss: 81.8567 - val_r2_keras: 0.5152\n",
      "Epoch 133/300\n",
      "3156/3156 [==============================] - 4s - loss: 74.9503 - r2_keras: 0.5512 - val_loss: 141.8690 - val_r2_keras: 0.0754\n",
      "Epoch 134/300\n",
      "3156/3156 [==============================] - 4s - loss: 75.2256 - r2_keras: 0.5543 - val_loss: 72.3380 - val_r2_keras: 0.5767\n",
      "Epoch 135/300\n",
      "3156/3156 [==============================] - 4s - loss: 76.2232 - r2_keras: 0.5497 - val_loss: 73.0233 - val_r2_keras: 0.5669\n",
      "Epoch 136/300\n",
      "3156/3156 [==============================] - 4s - loss: 75.9592 - r2_keras: 0.5450 - val_loss: 70.9823 - val_r2_keras: 0.5767\n",
      "Epoch 137/300\n",
      "3156/3156 [==============================] - 3s - loss: 77.2363 - r2_keras: 0.5348 - val_loss: 146.1160 - val_r2_keras: 0.0469\n",
      "Epoch 138/300\n",
      "3156/3156 [==============================] - 4s - loss: 75.6835 - r2_keras: 0.5494 - val_loss: 69.8287 - val_r2_keras: 0.5812\n",
      "Epoch 139/300\n",
      "3156/3156 [==============================] - 4s - loss: 74.6403 - r2_keras: 0.5642 - val_loss: 69.1577 - val_r2_keras: 0.5859\n",
      "Epoch 140/300\n",
      "3156/3156 [==============================] - 3s - loss: 77.1103 - r2_keras: 0.5326 - val_loss: 73.7781 - val_r2_keras: 0.5461\n",
      "Epoch 141/300\n",
      "3156/3156 [==============================] - 4s - loss: 75.9350 - r2_keras: 0.5431 - val_loss: 112.4572 - val_r2_keras: 0.2420\n",
      "Epoch 142/300\n",
      "3156/3156 [==============================] - 5s - loss: 77.3595 - r2_keras: 0.5293 - val_loss: 81.2254 - val_r2_keras: 0.5115\n",
      "Epoch 143/300\n",
      "3156/3156 [==============================] - 4s - loss: 75.3210 - r2_keras: 0.5559 - val_loss: 70.1468 - val_r2_keras: 0.5855\n",
      "Epoch 144/300\n",
      "3156/3156 [==============================] - 4s - loss: 77.3583 - r2_keras: 0.5352 - val_loss: 83.2706 - val_r2_keras: 0.4916\n",
      "Epoch 145/300\n",
      "3156/3156 [==============================] - 4s - loss: 76.1801 - r2_keras: 0.5542 - val_loss: 75.9760 - val_r2_keras: 0.54710.\n",
      "Epoch 146/300\n",
      "3156/3156 [==============================] - 4s - loss: 75.5603 - r2_keras: 0.5494 - val_loss: 72.7431 - val_r2_keras: 0.5589\n",
      "Epoch 147/300\n",
      "3156/3156 [==============================] - 4s - loss: 77.4244 - r2_keras: 0.5466 - val_loss: 84.9511 - val_r2_keras: 0.4908\n",
      "Epoch 148/300\n",
      "3156/3156 [==============================] - 4s - loss: 75.8609 - r2_keras: 0.5497 - val_loss: 77.3408 - val_r2_keras: 0.5324\n",
      "Epoch 149/300\n",
      "3156/3156 [==============================] - 4s - loss: 76.6381 - r2_keras: 0.5301 - val_loss: 73.0525 - val_r2_keras: 0.5689\n",
      "Epoch 150/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3156/3156 [==============================] - 3s - loss: 75.0924 - r2_keras: 0.5477 - val_loss: 122.4946 - val_r2_keras: 0.2228\n",
      "Epoch 151/300\n",
      "3156/3156 [==============================] - 3s - loss: 76.1957 - r2_keras: 0.5476 - val_loss: 98.7135 - val_r2_keras: 0.3967\n",
      "Epoch 152/300\n",
      "3156/3156 [==============================] - 3s - loss: 72.6654 - r2_keras: 0.5740 - val_loss: 69.0378 - val_r2_keras: 0.5913\n",
      "Epoch 153/300\n",
      "3156/3156 [==============================] - 3s - loss: 74.5055 - r2_keras: 0.5503 - val_loss: 71.9246 - val_r2_keras: 0.56050.\n",
      "Epoch 154/300\n",
      "3156/3156 [==============================] - 3s - loss: 73.5962 - r2_keras: 0.5635 - val_loss: 68.9852 - val_r2_keras: 0.5890\n",
      "Epoch 155/300\n",
      "3156/3156 [==============================] - 3s - loss: 73.8041 - r2_keras: 0.5643 - val_loss: 78.4221 - val_r2_keras: 0.5421\n",
      "Epoch 156/300\n",
      "3156/3156 [==============================] - 3s - loss: 73.1522 - r2_keras: 0.5731 - val_loss: 73.1234 - val_r2_keras: 0.5720\n",
      "Epoch 157/300\n",
      "3156/3156 [==============================] - 3s - loss: 74.6867 - r2_keras: 0.5488 - val_loss: 73.0117 - val_r2_keras: 0.5712\n",
      "Epoch 158/300\n",
      "3156/3156 [==============================] - 3s - loss: 75.3035 - r2_keras: 0.5624 - val_loss: 82.6637 - val_r2_keras: 0.5014\n",
      "Epoch 159/300\n",
      "3156/3156 [==============================] - 3s - loss: 76.1715 - r2_keras: 0.5481 - val_loss: 73.5515 - val_r2_keras: 0.5474\n",
      "Epoch 160/300\n",
      "3156/3156 [==============================] - 3s - loss: 74.5485 - r2_keras: 0.5509 - val_loss: 72.2314 - val_r2_keras: 0.57380\n",
      "Epoch 161/300\n",
      "3156/3156 [==============================] - 3s - loss: 75.0520 - r2_keras: 0.5588 - val_loss: 88.2302 - val_r2_keras: 0.4730\n",
      "Epoch 162/300\n",
      "3156/3156 [==============================] - 3s - loss: 75.4895 - r2_keras: 0.5577 - val_loss: 77.1984 - val_r2_keras: 0.5457\n",
      "Epoch 163/300\n",
      "3156/3156 [==============================] - 3s - loss: 75.2764 - r2_keras: 0.5572 - val_loss: 77.7509 - val_r2_keras: 0.5091\n",
      "Epoch 164/300\n",
      "3156/3156 [==============================] - 3s - loss: 73.5523 - r2_keras: 0.5631 - val_loss: 72.1699 - val_r2_keras: 0.5667\n",
      "Epoch 165/300\n",
      "3156/3156 [==============================] - 3s - loss: 75.3229 - r2_keras: 0.5492 - val_loss: 105.4325 - val_r2_keras: 0.3452\n",
      "Epoch 166/300\n",
      "3156/3156 [==============================] - 3s - loss: 73.7620 - r2_keras: 0.5766 - val_loss: 143.5596 - val_r2_keras: 0.0679\n",
      "Epoch 167/300\n",
      "3156/3156 [==============================] - 3s - loss: 74.9660 - r2_keras: 0.5534 - val_loss: 80.7172 - val_r2_keras: 0.5161\n",
      "Epoch 168/300\n",
      "3156/3156 [==============================] - 3s - loss: 73.4029 - r2_keras: 0.5774 - val_loss: 76.8154 - val_r2_keras: 0.5408\n",
      "Epoch 169/300\n",
      "3156/3156 [==============================] - 4s - loss: 72.6840 - r2_keras: 0.5675 - val_loss: 83.1774 - val_r2_keras: 0.4624\n",
      "Epoch 170/300\n",
      "3156/3156 [==============================] - 3s - loss: 76.3530 - r2_keras: 0.5415 - val_loss: 82.0027 - val_r2_keras: 0.4954\n",
      "Epoch 171/300\n",
      "3156/3156 [==============================] - 3s - loss: 73.6133 - r2_keras: 0.5673 - val_loss: 73.8402 - val_r2_keras: 0.5647\n",
      "Epoch 172/300\n",
      "3156/3156 [==============================] - 4s - loss: 73.9452 - r2_keras: 0.5548 - val_loss: 78.4802 - val_r2_keras: 0.5340\n",
      "Epoch 173/300\n",
      "3156/3156 [==============================] - 4s - loss: 73.0637 - r2_keras: 0.5663 - val_loss: 71.7529 - val_r2_keras: 0.5731\n",
      "Epoch 174/300\n",
      "3156/3156 [==============================] - 4s - loss: 73.9031 - r2_keras: 0.5614 - val_loss: 69.4715 - val_r2_keras: 0.5871\n",
      "Epoch 175/300\n",
      "3156/3156 [==============================] - 4s - loss: 73.2707 - r2_keras: 0.5689 - val_loss: 75.8296 - val_r2_keras: 0.5527\n",
      "Epoch 176/300\n",
      "3156/3156 [==============================] - 3s - loss: 72.5552 - r2_keras: 0.5810 - val_loss: 73.9844 - val_r2_keras: 0.5522\n",
      "Epoch 177/300\n",
      "3156/3156 [==============================] - 3s - loss: 74.0813 - r2_keras: 0.5606 - val_loss: 71.4872 - val_r2_keras: 0.5737\n",
      "Epoch 178/300\n",
      "3156/3156 [==============================] - 4s - loss: 74.7248 - r2_keras: 0.5488 - val_loss: 71.9252 - val_r2_keras: 0.5729\n",
      "Epoch 179/300\n",
      "3156/3156 [==============================] - 3s - loss: 72.7892 - r2_keras: 0.5730 - val_loss: 71.1270 - val_r2_keras: 0.5784\n",
      "Epoch 180/300\n",
      "3156/3156 [==============================] - 4s - loss: 73.0604 - r2_keras: 0.5782 - val_loss: 110.9533 - val_r2_keras: 0.2610\n",
      "Epoch 181/300\n",
      "3156/3156 [==============================] - 4s - loss: 75.9277 - r2_keras: 0.5503 - val_loss: 75.4808 - val_r2_keras: 0.5557\n",
      "Epoch 182/300\n",
      "3156/3156 [==============================] - 3s - loss: 75.0093 - r2_keras: 0.5448 - val_loss: 71.7238 - val_r2_keras: 0.5722\n",
      "Epoch 183/300\n",
      "3156/3156 [==============================] - 5s - loss: 73.2246 - r2_keras: 0.5577 - val_loss: 70.6199 - val_r2_keras: 0.57210.55\n",
      "Epoch 184/300\n",
      "3156/3156 [==============================] - 4s - loss: 74.1076 - r2_keras: 0.5536 - val_loss: 69.7867 - val_r2_keras: 0.5836\n",
      "Epoch 185/300\n",
      "3156/3156 [==============================] - 4s - loss: 75.8221 - r2_keras: 0.5428 - val_loss: 125.7835 - val_r2_keras: 0.1953\n",
      "Epoch 186/300\n",
      "3156/3156 [==============================] - 4s - loss: 73.0662 - r2_keras: 0.5704 - val_loss: 69.8239 - val_r2_keras: 0.5849\n",
      "Epoch 187/300\n",
      "3156/3156 [==============================] - 4s - loss: 76.0866 - r2_keras: 0.5511 - val_loss: 77.6690 - val_r2_keras: 0.5374\n",
      "Epoch 188/300\n",
      "3156/3156 [==============================] - 4s - loss: 73.2455 - r2_keras: 0.5695 - val_loss: 106.5412 - val_r2_keras: 0.3413- r2_keras: 0\n",
      "Epoch 189/300\n",
      "3156/3156 [==============================] - 4s - loss: 73.6368 - r2_keras: 0.5657 - val_loss: 69.2490 - val_r2_keras: 0.5833\n",
      "Epoch 190/300\n",
      "3156/3156 [==============================] - 4s - loss: 72.7410 - r2_keras: 0.5765 - val_loss: 73.7230 - val_r2_keras: 0.5656\n",
      "Epoch 191/300\n",
      "3156/3156 [==============================] - 4s - loss: 73.1486 - r2_keras: 0.5734 - val_loss: 69.7609 - val_r2_keras: 0.5825\n",
      "Epoch 192/300\n",
      "3156/3156 [==============================] - 4s - loss: 71.9242 - r2_keras: 0.5864 - val_loss: 81.4629 - val_r2_keras: 0.5076\n",
      "Epoch 193/300\n",
      "3156/3156 [==============================] - 4s - loss: 72.3760 - r2_keras: 0.5580 - val_loss: 115.7299 - val_r2_keras: 0.2696\n",
      "Epoch 194/300\n",
      "3156/3156 [==============================] - 4s - loss: 76.1618 - r2_keras: 0.5485 - val_loss: 73.3843 - val_r2_keras: 0.5566\n",
      "Epoch 195/300\n",
      "3156/3156 [==============================] - 4s - loss: 74.2174 - r2_keras: 0.5607 - val_loss: 135.7597 - val_r2_keras: 0.1312\n",
      "Epoch 196/300\n",
      "3156/3156 [==============================] - 4s - loss: 74.2847 - r2_keras: 0.5471 - val_loss: 74.3484 - val_r2_keras: 0.5653\n",
      "Epoch 197/300\n",
      "3156/3156 [==============================] - 4s - loss: 76.1085 - r2_keras: 0.5493 - val_loss: 134.3465 - val_r2_keras: 0.1404\n",
      "Epoch 198/300\n",
      "3156/3156 [==============================] - 4s - loss: 72.5004 - r2_keras: 0.5738 - val_loss: 131.3148 - val_r2_keras: 0.1595\n",
      "Epoch 199/300\n",
      "3156/3156 [==============================] - 4s - loss: 71.9855 - r2_keras: 0.5869 - val_loss: 77.5408 - val_r2_keras: 0.5219\n",
      "Epoch 200/300\n",
      "3156/3156 [==============================] - 4s - loss: 72.5146 - r2_keras: 0.5699 - val_loss: 80.8118 - val_r2_keras: 0.5138\n",
      "Epoch 201/300\n",
      "3156/3156 [==============================] - 3s - loss: 72.3564 - r2_keras: 0.5730 - val_loss: 78.9113 - val_r2_keras: 0.5290\n",
      "Epoch 202/300\n",
      "3156/3156 [==============================] - 4s - loss: 72.5336 - r2_keras: 0.5772 - val_loss: 84.1747 - val_r2_keras: 0.4875\n",
      "Epoch 203/300\n",
      "3156/3156 [==============================] - 4s - loss: 74.8417 - r2_keras: 0.5539 - val_loss: 75.7407 - val_r2_keras: 0.5526\n",
      "Epoch 204/300\n",
      "3156/3156 [==============================] - 4s - loss: 74.1292 - r2_keras: 0.5650 - val_loss: 70.6555 - val_r2_keras: 0.5792\n",
      "Epoch 205/300\n",
      "3156/3156 [==============================] - 4s - loss: 74.3610 - r2_keras: 0.5517 - val_loss: 70.6463 - val_r2_keras: 0.5731\n",
      "Epoch 206/300\n",
      "3156/3156 [==============================] - 5s - loss: 75.4160 - r2_keras: 0.5548 - val_loss: 71.1903 - val_r2_keras: 0.5794\n",
      "Epoch 207/300\n",
      "3156/3156 [==============================] - 4s - loss: 73.9472 - r2_keras: 0.5652 - val_loss: 78.1016 - val_r2_keras: 0.5189\n",
      "Epoch 208/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3156/3156 [==============================] - 4s - loss: 74.0064 - r2_keras: 0.5615 - val_loss: 79.2625 - val_r2_keras: 0.5216\n",
      "Epoch 209/300\n",
      "3156/3156 [==============================] - 4s - loss: 71.9908 - r2_keras: 0.5852 - val_loss: 72.2320 - val_r2_keras: 0.5687\n",
      "Epoch 210/300\n",
      "3156/3156 [==============================] - 4s - loss: 72.2060 - r2_keras: 0.5701 - val_loss: 71.3537 - val_r2_keras: 0.5842\n",
      "Epoch 211/300\n",
      "3156/3156 [==============================] - 4s - loss: 73.0838 - r2_keras: 0.5716 - val_loss: 83.1810 - val_r2_keras: 0.5055\n",
      "Epoch 212/300\n",
      "3156/3156 [==============================] - 4s - loss: 73.1333 - r2_keras: 0.5661 - val_loss: 78.4603 - val_r2_keras: 0.5269\n",
      "Epoch 213/300\n",
      "3156/3156 [==============================] - 4s - loss: 74.5343 - r2_keras: 0.5542 - val_loss: 88.7776 - val_r2_keras: 0.4688\n",
      "Epoch 214/300\n",
      "3156/3156 [==============================] - 4s - loss: 74.0295 - r2_keras: 0.5610 - val_loss: 269.2692 - val_r2_keras: -0.8561\n",
      "Epoch 215/300\n",
      "3156/3156 [==============================] - 4s - loss: 73.9782 - r2_keras: 0.5624 - val_loss: 81.5601 - val_r2_keras: 0.4833\n",
      "Epoch 216/300\n",
      "3156/3156 [==============================] - 4s - loss: 72.8242 - r2_keras: 0.5754 - val_loss: 133.1972 - val_r2_keras: 0.1519\n",
      "Epoch 217/300\n",
      "3156/3156 [==============================] - 4s - loss: 74.8782 - r2_keras: 0.5616 - val_loss: 75.9287 - val_r2_keras: 0.5441\n",
      "Epoch 218/300\n",
      "3156/3156 [==============================] - 4s - loss: 73.2638 - r2_keras: 0.5603 - val_loss: 75.0117 - val_r2_keras: 0.5613\n",
      "Epoch 219/300\n",
      "3156/3156 [==============================] - 4s - loss: 73.2234 - r2_keras: 0.5693 - val_loss: 75.8115 - val_r2_keras: 0.5533\n",
      "Epoch 220/300\n",
      "3156/3156 [==============================] - 4s - loss: 71.8842 - r2_keras: 0.5853 - val_loss: 70.0591 - val_r2_keras: 0.5843\n",
      "Epoch 221/300\n",
      "3156/3156 [==============================] - 4s - loss: 72.2142 - r2_keras: 0.5704 - val_loss: 140.5674 - val_r2_keras: 0.0964\n",
      "Epoch 222/300\n",
      "3156/3156 [==============================] - 3s - loss: 72.8555 - r2_keras: 0.5736 - val_loss: 72.4050 - val_r2_keras: 0.5734\n",
      "Epoch 223/300\n",
      "3156/3156 [==============================] - 3s - loss: 73.1965 - r2_keras: 0.5671 - val_loss: 77.3705 - val_r2_keras: 0.5464\n",
      "Epoch 224/300\n",
      "3156/3156 [==============================] - 3s - loss: 74.7236 - r2_keras: 0.5545 - val_loss: 150.9269 - val_r2_keras: 0.0171\n",
      "Epoch 225/300\n",
      "3156/3156 [==============================] - 3s - loss: 74.2836 - r2_keras: 0.5422 - val_loss: 75.4853 - val_r2_keras: 0.5462\n",
      "Epoch 226/300\n",
      "3156/3156 [==============================] - 3s - loss: 74.8222 - r2_keras: 0.5482 - val_loss: 83.6748 - val_r2_keras: 0.5016\n",
      "Epoch 227/300\n",
      "3156/3156 [==============================] - 3s - loss: 74.0259 - r2_keras: 0.5426 - val_loss: 86.2197 - val_r2_keras: 0.4781\n",
      "Epoch 228/300\n",
      "3156/3156 [==============================] - 3s - loss: 73.5593 - r2_keras: 0.5549 - val_loss: 91.6868 - val_r2_keras: 0.4422\n",
      "Epoch 229/300\n",
      "3156/3156 [==============================] - 3s - loss: 72.2754 - r2_keras: 0.5792 - val_loss: 92.2194 - val_r2_keras: 0.4396\n",
      "Epoch 230/300\n",
      "3156/3156 [==============================] - 3s - loss: 72.8363 - r2_keras: 0.5684 - val_loss: 82.7419 - val_r2_keras: 0.5097\n",
      "Epoch 231/300\n",
      "3156/3156 [==============================] - 3s - loss: 71.4775 - r2_keras: 0.5919 - val_loss: 76.3284 - val_r2_keras: 0.5491\n",
      "Epoch 232/300\n",
      "3156/3156 [==============================] - 3s - loss: 72.4970 - r2_keras: 0.5767 - val_loss: 78.2255 - val_r2_keras: 0.5353\n",
      "Epoch 233/300\n",
      "3156/3156 [==============================] - 3s - loss: 72.6281 - r2_keras: 0.5810 - val_loss: 71.1589 - val_r2_keras: 0.5801\n",
      "Epoch 234/300\n",
      "3156/3156 [==============================] - 3s - loss: 71.7810 - r2_keras: 0.5890 - val_loss: 76.1150 - val_r2_keras: 0.5471\n",
      "Epoch 235/300\n",
      "3156/3156 [==============================] - 3s - loss: 73.7559 - r2_keras: 0.5633 - val_loss: 112.5092 - val_r2_keras: 0.2875\n",
      "Epoch 236/300\n",
      "3156/3156 [==============================] - 3s - loss: 71.6755 - r2_keras: 0.5714 - val_loss: 70.6901 - val_r2_keras: 0.5735\n",
      "Epoch 237/300\n",
      "3156/3156 [==============================] - 3s - loss: 71.6678 - r2_keras: 0.5708 - val_loss: 69.9902 - val_r2_keras: 0.5845\n",
      "Epoch 238/300\n",
      "3156/3156 [==============================] - 3s - loss: 72.2522 - r2_keras: 0.5805 - val_loss: 73.2428 - val_r2_keras: 0.5707\n",
      "Epoch 239/300\n",
      "3156/3156 [==============================] - 3s - loss: 71.8666 - r2_keras: 0.5647 - val_loss: 76.3115 - val_r2_keras: 0.5275\n",
      "Epoch 240/300\n",
      "3156/3156 [==============================] - 3s - loss: 71.7962 - r2_keras: 0.5893 - val_loss: 87.6070 - val_r2_keras: 0.47060.5\n",
      "Epoch 241/300\n",
      "3156/3156 [==============================] - 3s - loss: 75.3636 - r2_keras: 0.5414 - val_loss: 82.4988 - val_r2_keras: 0.4805\n",
      "Epoch 242/300\n",
      "3156/3156 [==============================] - 3s - loss: 73.2184 - r2_keras: 0.5680 - val_loss: 74.6226 - val_r2_keras: 0.5578\n",
      "Epoch 243/300\n",
      "3156/3156 [==============================] - 3s - loss: 73.3019 - r2_keras: 0.5560 - val_loss: 80.8789 - val_r2_keras: 0.5111\n",
      "Epoch 244/300\n",
      "3156/3156 [==============================] - 3s - loss: 73.2333 - r2_keras: 0.5704 - val_loss: 110.0036 - val_r2_keras: 0.3148\n",
      "Epoch 245/300\n",
      "3156/3156 [==============================] - 3s - loss: 71.9100 - r2_keras: 0.5847 - val_loss: 106.4709 - val_r2_keras: 0.3386\n",
      "Epoch 246/300\n",
      "3156/3156 [==============================] - 3s - loss: 73.5677 - r2_keras: 0.5607 - val_loss: 71.3574 - val_r2_keras: 0.5703\n",
      "Epoch 247/300\n",
      "3156/3156 [==============================] - 3s - loss: 72.0949 - r2_keras: 0.5687 - val_loss: 82.7220 - val_r2_keras: 0.5157\n",
      "Epoch 248/300\n",
      "3156/3156 [==============================] - 4s - loss: 72.3231 - r2_keras: 0.5696 - val_loss: 72.6609 - val_r2_keras: 0.5625\n",
      "Epoch 249/300\n",
      "3156/3156 [==============================] - 3s - loss: 71.0175 - r2_keras: 0.5813 - val_loss: 77.2370 - val_r2_keras: 0.5230\n",
      "Epoch 250/300\n",
      "3156/3156 [==============================] - 4s - loss: 72.1113 - r2_keras: 0.5700 - val_loss: 79.9516 - val_r2_keras: 0.4908\n",
      "Epoch 251/300\n",
      "3156/3156 [==============================] - 3s - loss: 72.7832 - r2_keras: 0.5698 - val_loss: 73.6541 - val_r2_keras: 0.5583\n",
      "Epoch 252/300\n",
      "3156/3156 [==============================] - 3s - loss: 71.7429 - r2_keras: 0.5855 - val_loss: 71.1357 - val_r2_keras: 0.5765\n",
      "Epoch 253/300\n",
      "3156/3156 [==============================] - 3s - loss: 70.9187 - r2_keras: 0.5827 - val_loss: 70.3961 - val_r2_keras: 0.5850\n",
      "Epoch 254/300\n",
      "3156/3156 [==============================] - 3s - loss: 71.6006 - r2_keras: 0.5892 - val_loss: 74.2552 - val_r2_keras: 0.5525\n",
      "Epoch 255/300\n",
      "3156/3156 [==============================] - 3s - loss: 72.0651 - r2_keras: 0.5767 - val_loss: 75.4942 - val_r2_keras: 0.5462\n",
      "Epoch 256/300\n",
      "3156/3156 [==============================] - 4s - loss: 72.2408 - r2_keras: 0.5733 - val_loss: 74.9859 - val_r2_keras: 0.5616\n",
      "Epoch 257/300\n",
      "3156/3156 [==============================] - 4s - loss: 72.2429 - r2_keras: 0.5618 - val_loss: 84.3945 - val_r2_keras: 0.4989\n",
      "Epoch 258/300\n",
      "3156/3156 [==============================] - 3s - loss: 71.8717 - r2_keras: 0.5676 - val_loss: 75.5928 - val_r2_keras: 0.5531\n",
      "Epoch 259/300\n",
      "3156/3156 [==============================] - 3s - loss: 72.7189 - r2_keras: 0.5671 - val_loss: 72.8455 - val_r2_keras: 0.5677\n",
      "Epoch 260/300\n",
      "3156/3156 [==============================] - 3s - loss: 72.5574 - r2_keras: 0.5616 - val_loss: 97.0918 - val_r2_keras: 0.3979\n",
      "Epoch 261/300\n",
      "3156/3156 [==============================] - 3s - loss: 73.4247 - r2_keras: 0.5629 - val_loss: 72.8350 - val_r2_keras: 0.5680\n",
      "Epoch 262/300\n",
      "3156/3156 [==============================] - 3s - loss: 74.2575 - r2_keras: 0.5569 - val_loss: 70.5915 - val_r2_keras: 0.5821\n",
      "Epoch 263/300\n",
      "3156/3156 [==============================] - 3s - loss: 71.9991 - r2_keras: 0.5790 - val_loss: 72.0990 - val_r2_keras: 0.5667\n",
      "Epoch 264/300\n",
      "3156/3156 [==============================] - 3s - loss: 72.9202 - r2_keras: 0.5732 - val_loss: 70.0492 - val_r2_keras: 0.5856\n",
      "Epoch 265/300\n",
      "3156/3156 [==============================] - 3s - loss: 72.6682 - r2_keras: 0.5741 - val_loss: 76.9353 - val_r2_keras: 0.5414\n",
      "Epoch 266/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3156/3156 [==============================] - 3s - loss: 72.1721 - r2_keras: 0.5726 - val_loss: 69.7820 - val_r2_keras: 0.5823\n",
      "Epoch 267/300\n",
      "3156/3156 [==============================] - 4s - loss: 72.3214 - r2_keras: 0.5752 - val_loss: 70.4868 - val_r2_keras: 0.5807\n",
      "Epoch 268/300\n",
      "3156/3156 [==============================] - 4s - loss: 74.1890 - r2_keras: 0.5417 - val_loss: 70.6584 - val_r2_keras: 0.5787\n",
      "Epoch 269/300\n",
      "3156/3156 [==============================] - 3s - loss: 71.6955 - r2_keras: 0.5659 - val_loss: 74.4558 - val_r2_keras: 0.5431\n",
      "Epoch 270/300\n",
      "3156/3156 [==============================] - 4s - loss: 73.8683 - r2_keras: 0.5485 - val_loss: 70.5446 - val_r2_keras: 0.5703\n",
      "Epoch 271/300\n",
      "3156/3156 [==============================] - 3s - loss: 72.5191 - r2_keras: 0.5666 - val_loss: 73.2747 - val_r2_keras: 0.5716\n",
      "Epoch 272/300\n",
      "3156/3156 [==============================] - 3s - loss: 72.2101 - r2_keras: 0.5757 - val_loss: 84.9679 - val_r2_keras: 0.4628\n",
      "Epoch 273/300\n",
      "3156/3156 [==============================] - 3s - loss: 72.6956 - r2_keras: 0.5735 - val_loss: 70.3061 - val_r2_keras: 0.5840\n",
      "Epoch 274/300\n",
      "3156/3156 [==============================] - 3s - loss: 71.0572 - r2_keras: 0.5837 - val_loss: 73.5483 - val_r2_keras: 0.5775\n",
      "Epoch 275/300\n",
      "3156/3156 [==============================] - 3s - loss: 72.8228 - r2_keras: 0.5593 - val_loss: 70.0704 - val_r2_keras: 0.5827\n",
      "Epoch 276/300\n",
      "3156/3156 [==============================] - 3s - loss: 74.4021 - r2_keras: 0.5675 - val_loss: 71.3834 - val_r2_keras: 0.5802\n",
      "Epoch 277/300\n",
      "3156/3156 [==============================] - 3s - loss: 72.2332 - r2_keras: 0.5716 - val_loss: 70.9704 - val_r2_keras: 0.5849\n",
      "Epoch 278/300\n",
      "3156/3156 [==============================] - 4s - loss: 71.7310 - r2_keras: 0.5827 - val_loss: 85.6502 - val_r2_keras: 0.4871\n",
      "Epoch 279/300\n",
      "3156/3156 [==============================] - 3s - loss: 71.5555 - r2_keras: 0.5810 - val_loss: 77.1655 - val_r2_keras: 0.5397\n",
      "Epoch 280/300\n",
      "3156/3156 [==============================] - 3s - loss: 73.0284 - r2_keras: 0.5679 - val_loss: 73.7064 - val_r2_keras: 0.5582\n",
      "Epoch 281/300\n",
      "3156/3156 [==============================] - 3s - loss: 71.7666 - r2_keras: 0.5834 - val_loss: 69.5662 - val_r2_keras: 0.5852\n",
      "Epoch 282/300\n",
      "3156/3156 [==============================] - 4s - loss: 71.6449 - r2_keras: 0.5791 - val_loss: 151.8030 - val_r2_keras: 0.0054\n",
      "Epoch 283/300\n",
      "3156/3156 [==============================] - 4s - loss: 72.9935 - r2_keras: 0.5662 - val_loss: 92.4180 - val_r2_keras: 0.4417\n",
      "Epoch 284/300\n",
      "3156/3156 [==============================] - 3s - loss: 72.3122 - r2_keras: 0.5734 - val_loss: 74.8392 - val_r2_keras: 0.5604\n",
      "Epoch 285/300\n",
      "3156/3156 [==============================] - 3s - loss: 72.2814 - r2_keras: 0.5770 - val_loss: 70.6382 - val_r2_keras: 0.5768\n",
      "Epoch 286/300\n",
      "3156/3156 [==============================] - 3s - loss: 72.0042 - r2_keras: 0.5786 - val_loss: 70.8293 - val_r2_keras: 0.5818\n",
      "Epoch 287/300\n",
      "3156/3156 [==============================] - 3s - loss: 71.5525 - r2_keras: 0.5772 - val_loss: 75.2604 - val_r2_keras: 0.5563\n",
      "Epoch 288/300\n",
      "3156/3156 [==============================] - 3s - loss: 72.7103 - r2_keras: 0.5729 - val_loss: 68.3741 - val_r2_keras: 0.5906\n",
      "Epoch 289/300\n",
      "3156/3156 [==============================] - 3s - loss: 71.3864 - r2_keras: 0.5727 - val_loss: 71.3758 - val_r2_keras: 0.5794\n",
      "Epoch 290/300\n",
      "3156/3156 [==============================] - 3s - loss: 73.1238 - r2_keras: 0.5618 - val_loss: 69.7979 - val_r2_keras: 0.5830\n",
      "Epoch 291/300\n",
      "3156/3156 [==============================] - 3s - loss: 71.3018 - r2_keras: 0.5784 - val_loss: 70.6140 - val_r2_keras: 0.5863\n",
      "Epoch 292/300\n",
      "3156/3156 [==============================] - 3s - loss: 71.9758 - r2_keras: 0.5641 - val_loss: 68.6744 - val_r2_keras: 0.5906\n",
      "Epoch 293/300\n",
      "3156/3156 [==============================] - 3s - loss: 71.9040 - r2_keras: 0.5798 - val_loss: 73.6034 - val_r2_keras: 0.5582\n",
      "Epoch 294/300\n",
      "3156/3156 [==============================] - 4s - loss: 72.2942 - r2_keras: 0.5778 - val_loss: 70.3717 - val_r2_keras: 0.5729\n",
      "Epoch 295/300\n",
      "3156/3156 [==============================] - 3s - loss: 72.0178 - r2_keras: 0.5635 - val_loss: 71.3889 - val_r2_keras: 0.5720\n",
      "Epoch 296/300\n",
      "3156/3156 [==============================] - 3s - loss: 72.4485 - r2_keras: 0.5807 - val_loss: 70.6550 - val_r2_keras: 0.5866\n",
      "Epoch 297/300\n",
      "3156/3156 [==============================] - 4s - loss: 72.1233 - r2_keras: 0.5675 - val_loss: 69.8028 - val_r2_keras: 0.5822\n",
      "Epoch 298/300\n",
      "3156/3156 [==============================] - 4s - loss: 73.4659 - r2_keras: 0.5657 - val_loss: 73.0451 - val_r2_keras: 0.5659\n",
      "Epoch 299/300\n",
      "3156/3156 [==============================] - 5s - loss: 70.9924 - r2_keras: 0.5878 - val_loss: 85.6628 - val_r2_keras: 0.4822\n",
      "Epoch 300/300\n",
      "3156/3156 [==============================] - 5s - loss: 71.1864 - r2_keras: 0.5830 - val_loss: 79.7352 - val_r2_keras: 0.5021\n",
      "3140/3156 [============================>.] - ETA: 0s('\\ntrain : ', 0.51213461008054051)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'none' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-68c28342d253>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_feats_encode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-ed570ac94db6>\u001b[0m in \u001b[0;36mtrain_dl\u001b[0;34m(train_X, eval_X, train_y, eval_y)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\ntrain : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0meval_X\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0meval_y\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mprediction_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'none' is not defined"
     ]
    }
   ],
   "source": [
    "train_X, eval_X, train_y, eval_y = train_test_split(train_feats_encode, train_labels, random_state=42)\n",
    "train_dl(train_X, eval_X, train_y, eval_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_predictions = estimator.predict(test_feats_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = pd.DataFrame({'id': test['ID'].astype(np.int32), 'y': test_predictions})\n",
    "output.to_csv('results/keras/[%s][%s].csv' % (str(model.__class__.__name__),score), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
